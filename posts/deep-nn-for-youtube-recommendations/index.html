<!doctype html><html><head><meta charset=utf-8><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Paper summary on how to generate relevant YouTube recommendations"><base href=https://saikatkumardey.com/><title>Deep Neural Networks for YouTube Recommendations | Saikat Kumar Dey</title><link rel=canonical href=https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/><link href=https://saikatkumardey.com/css/style-v1.css type=text/css rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Alegreya&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Fira+Mono&display=swap" rel=stylesheet><meta name=twitter:card content="summary"><meta name=twitter:title content="Deep Neural Networks for YouTube Recommendations"><meta name=twitter:description content="Paper summary on how to generate relevant YouTube recommendations"><script async src="https://www.googletagmanager.com/gtag/js?id=G-X1SHK8DN16"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-X1SHK8DN16",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-X1SHK8DN16","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div class=container><header><h1><a href=https://saikatkumardey.com/>Saikat Kumar Dey</a></h1><nav><ul><li><a href=https://saikatkumardey.com/posts/ title>Posts</a></li><li><a href=https://saikatkumardey.com/projects/ title>Projects</a></li><li><a href=https://saikatkumardey.com/now/ title>Now</a></li></ul></nav></header><div id=content><main class=essay><h1 id=title>Deep Neural Networks for YouTube Recommendations</h1><div class=meta><span class=date>August 29, 2021</span></div><div><div class=toc><nav id=TableOfContents><ul><li><a href=https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/#the-problem>The Problem</a></li><li><a href=https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/#key-ideas>Key Ideas</a></li><li><a href=https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/#model-architecture>Model Architecture</a><ul><li><a href=https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/#candidate-generation>Candidate generation</a></li><li><a href=https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/#ranking>Ranking</a></li></ul></li><li><a href=https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/#references>References</a></li></ul></nav></div><article id=content><p><img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1629830470549/vXmBYRQ6P.png alt="Screenshot 2021-08-25 at 12.11.06 AM.png"></p><p>YouTube has 100m+ daily active users who consume more than a billion hours&rsquo; worth of content every day. 100s of hours of videos are uploaded every second. At that scale, recommending personalized videos is a colossal task.</p><p>I&rsquo;ve always wondered how YouTube is always able to come up with relevant recommendations that kept me hooked! I found a very interesting paper on <a href=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf>Deep Neural networks for YouTube Recommendations</a>. In this post, I will summarise the key ideas.</p><h2 id=the-problem>The Problem</h2><p>To able to come up with relevant & personalized recommendations for every user is a problem because of:</p><ul><li><strong>scale</strong>: billions of users, billions of videos.</li><li><strong>freshness</strong>: massive volume of videos are uploaded every day. It&rsquo;s an explore-exploit trade-off between popular vs new content.</li><li><strong>noise</strong>: only sparse implicit user feedback is available for modeling.</li></ul><p>In this paper, the authors demonstrate the usage of deep learning techniques for improving recommendations as opposed to matrix-factorization techniques used earlier.</p><h2 id=key-ideas>Key Ideas</h2><ul><li><p>The problem of recommendations at scale is divided into 2 subproblems:</p><ol><li><strong>Candidate Generation</strong> - selects a small subset from the overall corpus which might be relevant to the user.</li><li><strong>Ranking</strong> - ranks the candidate videos based on their relative importance.</li></ol></li><li><p>For Candidate Generation, the objective is to predict the next video watch. User search/watch history, demographics, etc are used by a simple feed-forward network as embeddings which are jointly learned during the training.</p></li><li><p>For Ranking, the objective is to model an expected watch time. A score is assigned based on the expected watch time and videos are sorted accordingly.</p></li><li><p>Similar neural network architecture is used for both procedures.</p></li><li><p>Offline metrics like precision, recall, ranking loss, etc. are used during development. A/B test is used to determine the final effectiveness of the model. We&rsquo;ve already explored the <a href=https://saikatkumardey.com/predictive-model-performance-offline-and-online-evaluations>discrepancies between offline vs online evaluation</a> in a different post.</p></li></ul><h2 id=model-architecture>Model Architecture</h2><h3 id=candidate-generation>Candidate generation</h3><p><img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1630224116536/RukzfmlR1.png alt="Screenshot 2021-08-29 at 1.31.51 PM.png"></p><ul><li><p>** Problem Formulation:** Recommendation is formulated as an <em>extreme multi-class classification</em>
P(w_t = i | U,C) = softmax(v_i . u)
where,
w_t = video v_i watched at time t
U = user
C = context
v_i = dense video embeddings
u = dense user embeddings</p><ul><li>The task of the deep neural network is to learn the embeddings <em>u</em> as a function of user history and context.</li><li>user completing a video watch is a positive example.</li><li><a href=https://www.tensorflow.org/extras/candidate_sampling.pdf>candidate sampling</a> & <a href=https://en.wikipedia.org/wiki/Importance_sampling>importance weighting</a> is used to sample negative examples.</li></ul></li><li><p>Embeddings describing the watch history, user query, demographics, etc are fed to a simple feed-forward neural network having a final softmax layer to learn the class probabilities.</p><ul><li><strong>watch history</strong>: dense vector representation of watched video is learned from a sequence of video-ids (just like word2vec).</li><li><strong>user query</strong>: n-gram representations</li><li><strong>demographics</strong>: geographic region, device, age, etc. are used as numerical/categorical features</li></ul></li><li><p>The embeddings are jointly learned while training the model via gradient descent back-propagation.</p></li><li><p>Age of the video is used to model the time-dependent nature of popular videos. Otherwise, the good old popular videos are going to be selected most of the times, isn&rsquo;t it?</p></li><li><p>What&rsquo;s interesting is that a lot of features were &ldquo;engineered&rdquo; as opposed to the promise of deep learning to reduce it.</p></li><li><p><strong>Training data & label:</strong></p><ul><li>There&rsquo;s an inherent sequence of video consumption. Hence, using random held-out data will be cheating since future information will leak into the training process. The model will overfit! Think about time-series forecasting. A random train-test split won&rsquo;t work since the future data is not available in production during serving time.
<img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1630225522918/zoyaMC-Ae.png alt="Screenshot 2021-08-29 at 1.55.18 PM.png"></li><li>The authors propose a model of predicting <em>user&rsquo;s next watch</em> instead of a randomly held-out watch. This makes sense, as we consume videos in a sequence. For example, if you&rsquo;re watching a series with several episodes, recommending a random episode from that series doesn&rsquo;t make sense.
<img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1630225542842/UAGeaWt8C.png alt="Screenshot 2021-08-29 at 1.55.39 PM.png"></li></ul></li><li><p><strong>Serving:</strong></p><ul><li>To score millions of videos in latency of tens of milliseconds, a nearest neighbor-based search algorithm is used. Exact probability values of softmax() are not required. Hence, a dot product of user and video embeddings could be used to figure out the propensity score of a user <em>u</em> for a particular video <em>v_i</em>. A nearest neighbor search algorithm could be used to figure out the top K candidate videos based on the score.</li></ul></li></ul><h3 id=ranking>Ranking</h3><p><img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1630229225282/zDkuTAg45.png alt="Screenshot 2021-08-29 at 2.56.54 PM.png"></p><ul><li>Candidate generation selects a few hundred out of millions of videos. The ranking procedure could make use of more video features as well as user&rsquo;s interactions with it in order to figure out an order of recommendation.</li><li>The model architecture is similar to the candidate generation procedure. We assign a score to the videos using weighted logistic regression.</li><li>The objective to optimize is a function of expected watch time per impression.</li><li>Why not click-through rate? Well, that would promote clickbait videos instead of quality content. Watch time is a better signal that captures engagement.</li><li><strong>Modeling Expected Watch Time</strong><ul><li><strong>Objective</strong>: Predict expected watch time for a given video.</li><li><strong>Model</strong>: Weighted Logistic Regression, since the class distributions are imbalanced.<ul><li><strong>Positive example</strong>: the video was watched.</li><li><strong>Negative example</strong>: the video was not clicked.</li></ul></li><li>**What are the weights in &ldquo;weighted&rdquo; logistic regression? **<ul><li>Positive examples are weighted by the watch time.</li><li>Negative examples are given a weight of 1.</li></ul></li><li><strong>Loss</strong>: cross-entropy</li></ul></li></ul><h2 id=references>References</h2><ol><li><a href=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf>Deep Neural Networks for YouTube Recommendations</a></li><li><a href=https://en.wikipedia.org/wiki/Softmax_function>Softmax</a></li></ol></article></div><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?text=Paper%20summary%20on%20how%20to%20generate%20relevant%20YouTube%20recommendations.&url=https%3a%2f%2fsaikatkumardey.com%2fposts%2fdeep-nn-for-youtube-recommendations%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5.0-4.55 2.04-4.55 4.54.0.36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3.0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35.0 12.92-6.92 12.92-12.93.0-.2.0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg></div></div></a><a class=resp-sharing-button__link href="mailto:?subject=Deep%20Neural%20Networks%20for%20YouTube%20Recommendations&body=https%3a%2f%2fsaikatkumardey.com%2fposts%2fdeep-nn-for-youtube-recommendations%2f" target=_self rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 4H2C.9 4 0 4.9.0 6v12c0 1.1.9 2 2 2h20c1.1.0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17.0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1.0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08.0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fsaikatkumardey.com%2fposts%2fdeep-nn-for-youtube-recommendations%2f&title=Deep%20Neural%20Networks%20for%20YouTube%20Recommendations&summary=Paper%20summary%20on%20how%20to%20generate%20relevant%20YouTube%20recommendations&source=https%3a%2f%2fsaikatkumardey.com%2fposts%2fdeep-nn-for-youtube-recommendations%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6.0 2.5 1 2.6 2.5.0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/></svg></div></div></a><a class=resp-sharing-button__link href="whatsapp://send?text=Deep%20Neural%20Networks%20for%20YouTube%20Recommendations https%3a%2f%2fsaikatkumardey.com%2fposts%2fdeep-nn-for-youtube-recommendations%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3.0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7.0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1.0-5.2 4.2-9.4 9.4-9.4 2.5.0 4.9 1 6.7 2.8s2.8 4.2 2.8 6.7c-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2s0-.5.1-.6.3-.3.4-.5c.2-.1.3-.3.4-.5s0-.4.0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z"/></svg></div></div><a class=resp-sharing-button__link href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fsaikatkumardey.com%2fposts%2fdeep-nn-for-youtube-recommendations%2f&t=Deep%20Neural%20Networks%20for%20YouTube%20Recommendations" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 140 140"><path fill-rule="evenodd" d="M60.94 82.314 17 0h20.08l25.85 52.093c.397.927.86 1.888 1.39 2.883.53.994.995 2.02 1.393 3.08.265.4.463.764.596 1.095.13.334.262.63.395.898.662 1.325 1.26 2.618 1.79 3.877.53 1.26.993 2.42 1.39 3.48 1.06-2.254 2.22-4.673 3.48-7.258s2.552-5.27 3.877-8.052L103.49.0h18.69L77.84 83.308v53.087h-16.9v-54.08z"/></svg></div></div></a></a></main></div><footer><div><p><div class=social-icons><a href=https://www.linkedin.com/in/saikatkumardey/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a><a href=https://github.com/saikatkumardey/ target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=https://twitter.com/deysaikatkumar/ target=_blank rel="noopener noreferrer me" title=Twitter><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a><a href=https://www.instagram.com/saikatkumardey/ target=_blank rel="noopener noreferrer me" title=Instagram><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg></a></div>Powered by <a hre=https://gohugo.io/>Hugo</a>, theme <a href=https://github.com/caramelomartins/hugo-theme-candy>Candy</a></p></div></footer></div></body></html>