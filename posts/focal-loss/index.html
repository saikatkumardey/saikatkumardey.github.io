<!doctype html><html><head><meta charset=utf-8><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Focal loss is an effective technique for addressing class imbalance in machine learning by weighting the loss function to focus on hard-to-classify examples."><base href=https://saikatkumardey.com/><title>Solving Class Imbalance with Focal Loss | Saikat Kumar Dey</title><link rel=canonical href=https://saikatkumardey.com/posts/focal-loss/><link href=https://saikatkumardey.com/css/style-v1.css type=text/css rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Alegreya&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Fira+Mono&display=swap" rel=stylesheet><meta name=twitter:card content="summary"><meta name=twitter:title content="Solving Class Imbalance with Focal Loss"><meta name=twitter:description content="Focal loss is an effective technique for addressing class imbalance in machine learning by weighting the loss function to focus on hard-to-classify examples."><script async src="https://www.googletagmanager.com/gtag/js?id=G-X1SHK8DN16"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-X1SHK8DN16",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-X1SHK8DN16","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div class=container><header><h1><a href=https://saikatkumardey.com/>Saikat Kumar Dey</a></h1><nav><ul><li><a href=https://saikatkumardey.com/posts/ title>Posts</a></li><li><a href=https://saikatkumardey.com/projects/ title>Projects</a></li><li><a href=https://saikatkumardey.com/now/ title>Now</a></li></ul></nav></header><div id=content><main class=essay><h1 id=title>Solving Class Imbalance with Focal Loss</h1><div class=meta><span class=date>December 1, 2022</span></div><div><div class=toc><nav id=TableOfContents><ul><li><a href=https://saikatkumardey.com/posts/focal-loss/#focal-loss>Focal loss</a></li><li><a href=https://saikatkumardey.com/posts/focal-loss/#focal-loss-parameters---alpha-and-gamma>Focal loss parameters - alpha and gamma</a></li><li><a href=https://saikatkumardey.com/posts/focal-loss/#code-sample>Code sample</a></li><li><a href=https://saikatkumardey.com/posts/focal-loss/#thoughts-on-parameter-tuning>Thoughts on parameter tuning</a></li></ul><ul><li><a href=https://saikatkumardey.com/posts/focal-loss/#example>Example</a></li><li><a href=https://saikatkumardey.com/posts/focal-loss/#conclusion>Conclusion</a></li></ul></nav></div><article id=content><p>Class imbalance occurs when the number of observations in one or more classes is significantly different from the number of observations in other classes. This can lead to poor model performance, particularly when the minority class is important to predict accurately.</p><p>Focal loss is a loss function that down-weights the contribution of well-classified examples and focuses on hard-to-classify examples. This can help to improve the performance of a machine learning model when dealing with class imbalance.</p><p>In this post, we will understand the concept of focal loss and how it can be implemented in a machine learning model. We will also compare its effectiveness to other methods for solving class imbalance.</p><h2 id=focal-loss>Focal loss</h2><p>Focal loss is a variant of the cross-entropy loss function that is specifically designed to address class imbalance. It is defined as:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>FL(p) = -alpha * (1-p)^gamma * log(p)
</span></span></code></pre></div><p>where</p><ul><li>p is the predicted probability of the correct class</li><li>alpha is a weighting factor for each sample,</li><li>gamma is a tunable focusing parameter.</li></ul><p>The focal loss function has the effect of down-weighting well-classified examples, and up-weighting examples that are misclassified. This can be useful in class imbalance scenarios, where the model may be inclined to simply predict the majority class for all examples in order to achieve a high overall accuracy. By using focal loss, the model is encouraged to focus on the hard examples, and to learn to predict the minority class more accurately.</p><h2 id=focal-loss-parameters---alpha-and-gamma>Focal loss parameters - alpha and gamma</h2><p>In the focal loss formula, alpha and gamma are two tunable parameters that control the behavior of the loss function.</p><p><code>alpha</code> is a weighting factor that is applied to each sample in the batch. It has the effect of down-weighting well-classified examples and up-weighting examples that are misclassified. The value of alpha is typically set such that the majority class has a lower weighting than the minority class, to help the model focus on the hard examples.</p><p><code>gamma</code> is a focusing parameter that controls the degree to which the loss is down-weighted for well-classified examples. The value of gamma is typically set to a value greater than 1, which has the effect of down-weighting the loss for well-classified examples more heavily. This can help the model to focus on the hard examples and to learn to predict the minority class more accurately.</p><h2 id=code-sample>Code sample</h2><p>Here is an example of focal loss implemented in PyTorch:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a90d91>import</span> <span style=color:#000>torch</span>
</span></span><span style=display:flex><span><span style=color:#a90d91>def</span> <span style=color:#000>focal_loss</span>(<span style=color:#000>logits</span>, <span style=color:#000>targets</span>, <span style=color:#000>alpha</span>, <span style=color:#000>gamma</span>):
</span></span><span style=display:flex><span>    <span style=color:#000>probs</span> <span style=color:#000>=</span> <span style=color:#000>torch</span><span style=color:#000>.</span><span style=color:#000>sigmoid</span>(<span style=color:#000>logits</span>)
</span></span><span style=display:flex><span>    <span style=color:#000>weights</span> <span style=color:#000>=</span> <span style=color:#000>alpha</span> <span style=color:#000>*</span> (<span style=color:#1c01ce>1</span> <span style=color:#000>-</span> <span style=color:#000>probs</span>) <span style=color:#000>**</span> <span style=color:#000>gamma</span>
</span></span><span style=display:flex><span>    <span style=color:#000>focal_loss</span> <span style=color:#000>=</span> <span style=color:#000>-</span><span style=color:#000>weights</span> <span style=color:#000>*</span> <span style=color:#000>torch</span><span style=color:#000>.</span><span style=color:#000>log</span>(<span style=color:#000>probs</span>)
</span></span><span style=display:flex><span>    <span style=color:#a90d91>return</span> <span style=color:#000>focal_loss</span><span style=color:#000>.</span><span style=color:#000>mean</span>()
</span></span></code></pre></div><p>This function takes the model&rsquo;s predicted logits, the true targets, and the values of <code>alpha</code> and <code>gamma</code> as input and returns the average focal loss. It can then be used in the training loop of a machine learning model as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#177500># Train the model</span>
</span></span><span style=display:flex><span><span style=color:#a90d91>for</span> <span style=color:#000>inputs</span>, <span style=color:#000>targets</span> <span style=color:#000>in</span> <span style=color:#000>train_loader</span>:
</span></span><span style=display:flex><span>    <span style=color:#177500># Calculate predicted logits</span>
</span></span><span style=display:flex><span>    <span style=color:#000>logits</span> <span style=color:#000>=</span> <span style=color:#000>model</span>(<span style=color:#000>inputs</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#177500># Calculate focal loss</span>
</span></span><span style=display:flex><span>    <span style=color:#000>loss</span> <span style=color:#000>=</span> <span style=color:#000>focal_loss</span>(<span style=color:#000>logits</span>, <span style=color:#000>targets</span>, <span style=color:#000>alpha</span><span style=color:#000>=</span><span style=color:#1c01ce>0.25</span>, <span style=color:#000>gamma</span><span style=color:#000>=</span><span style=color:#1c01ce>2.0</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#177500># Backpropagate and update the model weights</span>
</span></span><span style=display:flex><span>    <span style=color:#000>loss</span><span style=color:#000>.</span><span style=color:#000>backward</span>()
</span></span><span style=display:flex><span>    <span style=color:#000>optimizer</span><span style=color:#000>.</span><span style=color:#000>step</span>()
</span></span></code></pre></div><p>In this example, we use the focal loss function to calculate the loss for each batch of training data and update the model weights using backpropagation. The values of <code>alpha</code> and <code>gamma</code> can be adjusted as needed to achieve the best performance.</p><h2 id=thoughts-on-parameter-tuning>Thoughts on parameter tuning</h2><p>In the example above, the focal loss function is defined as a partial function, with alpha and gamma as fixed parameters. The model is trained using the Adam optimizer, and the focal loss is used as the criterion. By adjusting the values of <code>alpha</code> and <code>gamma</code>, you can tune the focal loss function to suit your specific dataset and task.</p><p>The values of alpha and gamma can be tuned to suit the specific dataset and task. In our example, we used <code>alpha=0.75</code> and <code>gamma=2.0</code>, but these values may not be optimal for all datasets. By experimenting with different values of these parameters, you can find the settings that produce the best results for your model.</p><p>For example, if you increase the value of alpha, the weighting of the majority class will be decreased, and the weighting of the minority class will be increased. This can help the model to focus more on the minority class, and to learn to predict it more accurately. Similarly, if you increase the value of gamma, the down-weighting of well-classified examples will be increased, which can also help the model to focus more on the hard examples.</p><h1 id=comparison-with-other-methods>Comparison with Other Methods</h1><table><thead><tr><th>Method</th><th>Description</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td>Undersampling</td><td>Removing examples from the majority class to balance the distribution of the training data</td><td>Can be effective in reducing class imbalance</td><td>Can lead to loss of important information from the majority class</td></tr><tr><td>Oversampling</td><td>Adding synthetic examples to the minority class to balance the distribution of the training data</td><td>Can be effective in reducing class imbalance</td><td>Can lead to overfitting if synthetic examples are not representative of the true data distribution</td></tr><tr><td>Focal Loss</td><td>Loss function that down-weights the contribution of well-classified examples and focuses on hard-to-classify examples</td><td>Does not require changing the distribution of the training data</td><td>Requires tuning of hyperparameters to achieve optimal performance</td></tr></tbody></table><p>As shown in the table, focal loss has several advantages over undersampling and oversampling for dealing with class imbalance. It does not require changing the distribution of the training data, and it can be easily implemented in any machine learning model. However, it does require tuning of the hyperparameters <code>alpha</code> and <code>gamma</code> to achieve optimal performance.</p><h2 id=example>Example</h2><p>In this example, we will demonstrate the effectiveness of focal loss for solving class imbalance in a machine learning model. We will use a real-world example of a binary classification problem to illustrate how focal loss can improve the performance of a model on an imbalanced dataset.</p><p>Class imbalance is a common problem in medical imaging applications where the number of positive examples (e.g. diseased images) is often much smaller than the number of negative examples (e.g. healthy images). In this case study, we will consider a dataset of medical images and a binary classification task of predicting whether an image is healthy or diseased. The dataset is imbalanced, with a large number of healthy images and a small number of diseased images.</p><p>To start, we will train a convolutional neural network (CNN) using the cross-entropy loss as the training loss. This loss function is commonly used in classification tasks, as it measures the difference between the predicted probabilities and the true labels. However, it has the disadvantage of treating all classes equally, which can result in poor performance on imbalanced datasets.</p><table><thead><tr><th>Image</th><th>True Label</th><th>Healthy</th><th>Diseased</th><th>Cross-Entropy Loss</th><th>Focal Loss</th></tr></thead><tbody><tr><td>1</td><td>Healthy</td><td>0.8</td><td>0.2</td><td>0.2231435</td><td>0.0436035</td></tr><tr><td>2</td><td>Healthy</td><td>0.9</td><td>0.1</td><td>0.1053605</td><td>0.0160707</td></tr><tr><td>3</td><td>Healthy</td><td>0.7</td><td>0.3</td><td>0.3566749</td><td>0.0479158</td></tr><tr><td>4</td><td>Healthy</td><td>0.6</td><td>0.4</td><td>0.5150763</td><td>0.0687455</td></tr><tr><td>5</td><td>Diseased</td><td>0.1</td><td>0.9</td><td>0.1053605</td><td>0.8</td></tr></tbody></table><p>As we can see, the focal loss for the diseased image is significantly higher than the loss for the healthy images. This is because the focal loss down-weights the contribution of well-classified examples and focuses on hard-to-classify examples. In this case, the healthy images are well-classified and have a low focal loss, while the diseased image is hard-to-classify and has a high focal loss. This helps the model to better learn the patterns in the minority class of diseased images.</p><h2 id=conclusion>Conclusion</h2><p>In this blog post, we explored the problem of class imbalance in machine learning and introduced focal loss as a potential solution. We discussed the motivations behind focal loss and provided an example of how to implement it in a machine learning model. We also compared focal loss to other methods for solving class imbalance, such as undersampling and oversampling. Finally, we provided a case study of using focal loss to improve the performance of a machine learning model with class imbalance.</p><p>Overall, focal loss is an effective technique for addressing class imbalance in machine learning. It can improve the performance of models by weighting the loss function to focus on hard-to-classify examples, while still allowing easy examples to contribute to the learning process.</p><p>If you&rsquo;re interested in learning more about focal loss and other methods for solving class imbalance, there are many resources available online. Some suggested readings include:</p><ul><li><a href=https://arxiv.org/abs/1708.02002>Focal Loss for Dense Object Detection</a></li><li><a href=https://hasty.ai/docs/mp-wiki/loss/focal-loss>Focal Loss</a></li><li><a href=https://www.analyticsvidhya.com/blog/2020/08/a-beginners-guide-to-focal-loss-in-object-detection/>A Beginner&rsquo;s Guide To Focal Loss In Object Detection</a></li></ul></article></div><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?text=Focal%20loss%20is%20an%20effective%20technique%20for%20addressing%20class%20imbalance%20in%20machine%20learning%20by%20weighting%20the%20loss%20function%20to%20focus%20on%20hard-to-classify%20examples..&amp;url=https%3a%2f%2fsaikatkumardey.com%2fposts%2ffocal-loss%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5.0-4.55 2.04-4.55 4.54.0.36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3.0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35.0 12.92-6.92 12.92-12.93.0-.2.0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg></div></div></a><a class=resp-sharing-button__link href="mailto:?subject=Solving%20Class%20Imbalance%20with%20Focal%20Loss&amp;body=https%3a%2f%2fsaikatkumardey.com%2fposts%2ffocal-loss%2f" target=_self rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 4H2C.9 4 0 4.9.0 6v12c0 1.1.9 2 2 2h20c1.1.0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17.0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1.0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08.0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsaikatkumardey.com%2fposts%2ffocal-loss%2f&amp;title=Solving%20Class%20Imbalance%20with%20Focal%20Loss&amp;summary=Focal%20loss%20is%20an%20effective%20technique%20for%20addressing%20class%20imbalance%20in%20machine%20learning%20by%20weighting%20the%20loss%20function%20to%20focus%20on%20hard-to-classify%20examples.&amp;source=https%3a%2f%2fsaikatkumardey.com%2fposts%2ffocal-loss%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6.0 2.5 1 2.6 2.5.0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/></svg></div></div></a><a class=resp-sharing-button__link href="whatsapp://send?text=Solving%20Class%20Imbalance%20with%20Focal%20Loss https%3a%2f%2fsaikatkumardey.com%2fposts%2ffocal-loss%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3.0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7.0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1.0-5.2 4.2-9.4 9.4-9.4 2.5.0 4.9 1 6.7 2.8s2.8 4.2 2.8 6.7c-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2s0-.5.1-.6.3-.3.4-.5c.2-.1.3-.3.4-.5s0-.4.0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z"/></svg></div></div><a class=resp-sharing-button__link href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fsaikatkumardey.com%2fposts%2ffocal-loss%2f&amp;t=Solving%20Class%20Imbalance%20with%20Focal%20Loss" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 140 140"><path fill-rule="evenodd" d="M60.94 82.314 17 0h20.08l25.85 52.093c.397.927.86 1.888 1.39 2.883.53.994.995 2.02 1.393 3.08.265.4.463.764.596 1.095.13.334.262.63.395.898.662 1.325 1.26 2.618 1.79 3.877.53 1.26.993 2.42 1.39 3.48 1.06-2.254 2.22-4.673 3.48-7.258s2.552-5.27 3.877-8.052L103.49.0h18.69L77.84 83.308v53.087h-16.9v-54.08z"/></svg></div></div></a></a></main></div><footer><div><p><div class=social-icons><a href=https://www.linkedin.com/in/saikatkumardey/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a><a href=https://github.com/saikatkumardey/ target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=https://twitter.com/deysaikatkumar/ target=_blank rel="noopener noreferrer me" title=Twitter><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a><a href=https://www.instagram.com/saikatkumardey/ target=_blank rel="noopener noreferrer me" title=Instagram><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg></a></div>Powered by <a hre=https://gohugo.io/>Hugo</a>, theme <a href=https://github.com/caramelomartins/hugo-theme-candy>Candy</a></p></div></footer></div></body></html>