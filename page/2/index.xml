<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Saikat Kumar Dey</title><link>https://saikatkumardey.com/</link><description>Saikat Kumar Dey</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 29 May 2022 22:54:52 +0530</lastBuildDate><atom:link href="https://saikatkumardey.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Predictive Model Performance Online and Offline Eval</title><link>https://saikatkumardey.com/essays/predictive-model-performance-online-and-offline-eval/</link><pubDate>Wed, 25 Aug 2021 09:57:00 +0530</pubDate><guid>https://saikatkumardey.com/essays/predictive-model-performance-online-and-offline-eval/</guid><description>&lt;p>Evaluation is an important topic in every machine learning project. There are offline metrics that we compute on the historical data. It&amp;rsquo;s supposed to provide us an indication of our model performance on real data. However, we often see a discrepancy in offline vs online performance.&lt;/p>
&lt;p>In &lt;a href="https://dl.acm.org/doi/abs/10.1145/2487575.2488215">Predictive model performance: Offline and online evaluations&lt;/a>, the authors investigate the offline/online model performance on advertisement data from Bing Search Engine.&lt;/p>
&lt;h2 id="key-takeaways">Key takeaways&lt;/h2>
&lt;ul>
&lt;li>Evaluation metrics are important since it guides what the model optimizes on. Tuning on incorrect metrics might provide misleading results in offline settings that might surprise us in production.&lt;/li>
&lt;li>AUC is a really good metric to determine model classification efficiency.&lt;/li>
&lt;li>Offline evaluation using AUC doesn&amp;rsquo;t correlate to the online evaluation via A/B tests.&lt;/li>
&lt;li>The authors propose the usage of a simulation metric that simulates user behavior based on historical logs, which works better in the online evaluation.&lt;/li>
&lt;/ul>
&lt;h2 id="details">Details&lt;/h2>
&lt;p>&lt;strong>What is an offline evaluation?&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>In typical ML projects, we split our dataset into train/test sets.&lt;/li>
&lt;li>Models are trained on the train set.&lt;/li>
&lt;li>Evaluation is done on the test set.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What is an online evaluation?&lt;/strong>&lt;/p>
&lt;p>When our model is in production, we perform an A/B test.
Typically, it has 2 variants.&lt;/p>
&lt;ul>
&lt;li>&lt;em>control&lt;/em> group with our existing model&lt;/li>
&lt;li>&lt;em>test&lt;/em> group with the new model.
Live traffic is split into the two groups &amp;amp; metrics like conversion rate, revenue per visitor, etc are measured. If the new model&amp;rsquo;s performance is statistically significant, it&amp;rsquo;s selected for launch.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>The issue?&lt;/strong>&lt;/p>
&lt;p>Offline performance doesn&amp;rsquo;t always correlate to online performance due to the dynamic nature of the latter.&lt;/p>
&lt;h2 id="evaluation-metrics">Evaluation Metrics&lt;/h2>
&lt;ul>
&lt;li>The paper focuses on metrics used for click prediction problems that most search engines like Google, Bing, etc. face.&lt;/li>
&lt;li>Click prediction problems estimates the CTRs of ads given a query.&lt;/li>
&lt;li>This is treated as a binary classification problem.&lt;/li>
&lt;/ul>
&lt;p>We&amp;rsquo;ll review some important evaluation metrics for the use case.&lt;/p>
&lt;h3 id="auc-area-under-curve">AUC (Area under curve)&lt;/h3>
&lt;ul>
&lt;li>Let&amp;rsquo;s say that we have a binary classifier that predicts a probability &lt;em>p&lt;/em> for an event to occur. Then, &lt;em>1-p&lt;/em> is the probability that the event doesn&amp;rsquo;t occur. We need a threshold to determine the class membership. AUC provides a single score that tells us how good a model is across all possible ranges of thresholds.&lt;/li>
&lt;li>AUC is computed from a ROC (Receiver Operating Characteristics) curve.&lt;/li>
&lt;li>ROC curve = a graphical representation of TPR (true positive rate) as a function of FPR (false positive rate) of a binary classifier across different thresholds.&lt;/li>
&lt;/ul>
&lt;h3 id="rig-relative-information-gain">RIG (Relative Information Gain)&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>RIG = 1 - log_loss/entropy(y)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>where,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>log_loss = - [ c*log(p) + (1-c)*log(1-p) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>entropy(y) = - [ y*logy + (1-y)*log(1-y) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>c = observed click
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>p = probability of a click
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y = CTR
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Higher is better.&lt;/p>
&lt;h3 id="prediction-error-pe">Prediction Error (PE)&lt;/h3>
&lt;p>&lt;code>PE = avg(p)/y - 1&lt;/code>&lt;/p>
&lt;ul>
&lt;li>PE = 0 when average(p) exactly matches the click-through rate.&lt;/li>
&lt;li>It could also be 0 if there&amp;rsquo;s a mix of over-estimation/under-estimation of the CTR as long the average is closer to CTR.&lt;/li>
&lt;li>It&amp;rsquo;s not a reliable metric.&lt;/li>
&lt;/ul>
&lt;h3 id="simulation-metric">Simulation Metric&lt;/h3>
&lt;p>This part is really important. It teaches us a way to simulate different model performances offline without having to run expensive A/B tests.&lt;/p>
&lt;ul>
&lt;li>A/B tests run with a fixed set of model parameters.
&lt;ul>
&lt;li>It could be expensive to run multiple experiments with different model parameters.&lt;/li>
&lt;li>It could also ruin the user experience, make losses if the new model underperforms&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The paper proposes a simulation of the user behavior offline aka auction simulation.&lt;/li>
&lt;li>Auction simulation reruns ad auctions offline for a given query and selects a set of ads based on the new model prediction scores.&lt;/li>
&lt;li>user clicks are estimated in the following way:
&lt;ul>
&lt;li>if (user, ad) pair is found in the logs
&lt;ul>
&lt;li>if it&amp;rsquo;s in the same position in history as in the simulation, use the historic CTR directly as the expected CTR&lt;/li>
&lt;li>if it&amp;rsquo;s not in the same position, the expected CTR is calibrated based on the position.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>if (user, ad) pair is not found, average CTR is used as the expected CTR.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="issues">Issues&lt;/h2>
&lt;h3 id="auc">AUC&lt;/h3>
&lt;ul>
&lt;li>ignores predicted probability values. It&amp;rsquo;s insensitive to the ranking based on the probability score. It&amp;rsquo;s possible to have different rankings with similar AUC scores.&lt;/li>
&lt;li>summarizes the test performance over the entire range of the ROC space, even where one would rarely operate on. Higher ROC doesn&amp;rsquo;t mean a better ranking.&lt;/li>
&lt;li>It weights false-positive and false negatives equally. In real life, the cost of not showing a relevant ad (false negatives) is way more than showing a sub-optimal ad (false positive).&lt;/li>
&lt;li>highly dependent on the underlying data distribution.&lt;/li>
&lt;/ul>
&lt;h3 id="rig">RIG&lt;/h3>
&lt;ul>
&lt;li>Highly sensitive to underlying data distribution.&lt;/li>
&lt;li>We can&amp;rsquo;t judge a model by just using RIG alone.&lt;/li>
&lt;li>We could compare the relative performance of different models trained/tested on the same data.&lt;/li>
&lt;/ul>
&lt;h2 id="offline-vs-online-discrepancy">Offline vs Online discrepancy&lt;/h2>
&lt;p>The authors compare 2 models&lt;/p>
&lt;ul>
&lt;li>model 1 (baseline): tuned on offline metrics like AUC &amp;amp; RIG&lt;/li>
&lt;li>model 2 (test): tuned on the simulation metric&lt;/li>
&lt;/ul>
&lt;p>The finding: model performs well on offline metrics but has a significant dip on online metrics.&lt;/p>
&lt;p>&lt;strong>Why do we see this?&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1629743092351/g_2-BgJ8G.png" alt="Screenshot 2021-08-23 at 11.54.47 PM.png">&lt;/p>
&lt;ul>
&lt;li>Tuning a model on offline metrics like AUC/RIG over-estimates the probability scores at the lower end of the score range.&lt;/li>
&lt;li>Over-estimation of the probability score at the higher end of the score range doesn&amp;rsquo;t matter much since they&amp;rsquo;ll be selected by either model.&lt;/li>
&lt;li>Over-estimation at the lower end of the score range is bad since irrelevant ads are more likely to be shown in that case.&lt;/li>
&lt;li>Offline metrics like AUC/RIG provide an overall score based on the entire range of probability scores - they&amp;rsquo;re not able to capture the intended effect.&lt;/li>
&lt;li>Tuning a model based on the simulation metric correlates better with online performance tests via A/B tests.&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2>
&lt;p>&lt;a href="https://dl.acm.org/doi/abs/10.1145/2487575.2488215">Predictive Model Performance: Offline and Online Evaluations&lt;/a>&lt;/p></description></item></channel></rss>