<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Saikat Kumar Dey</title><link>https://saikatkumardey.com/</link><description>Saikat Kumar Dey</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 15 Jun 2022 19:26:47 +0530</lastBuildDate><atom:link href="https://saikatkumardey.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Deep Neural Networks for YouTube Recommendations</title><link>https://saikatkumardey.com/essays/deep-nn-for-youtube-recommendations/</link><pubDate>Sun, 29 Aug 2021 09:59:03 +0530</pubDate><guid>https://saikatkumardey.com/essays/deep-nn-for-youtube-recommendations/</guid><description>&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1629830470549/vXmBYRQ6P.png" alt="Screenshot 2021-08-25 at 12.11.06 AM.png">&lt;/p>
&lt;p>YouTube has 100m+ daily active users who consume more than a billion hours&amp;rsquo; worth of content every day. 100s of hours of videos are uploaded every second. At that scale, recommending personalized videos is a colossal task.&lt;/p>
&lt;p>I&amp;rsquo;ve always wondered how YouTube is always able to come up with relevant recommendations that kept me hooked! I found a very interesting paper on &lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf">Deep Neural networks for YouTube Recommendations&lt;/a>. In this post, I will summarise the key ideas.&lt;/p>
&lt;h2 id="the-problem">The Problem&lt;/h2>
&lt;p>To able to come up with relevant &amp;amp; personalized recommendations for every user is a problem because of:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>scale&lt;/strong>: billions of users, billions of videos.&lt;/li>
&lt;li>&lt;strong>freshness&lt;/strong>: massive volume of videos are uploaded every day. It&amp;rsquo;s an explore-exploit trade-off between popular vs new content.&lt;/li>
&lt;li>&lt;strong>noise&lt;/strong>: only sparse implicit user feedback is available for modeling.&lt;/li>
&lt;/ul>
&lt;p>In this paper, the authors demonstrate the usage of deep learning techniques for improving recommendations as opposed to matrix-factorization techniques used earlier.&lt;/p>
&lt;h2 id="key-ideas">Key Ideas&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>The problem of recommendations at scale is divided into 2 subproblems:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Candidate Generation&lt;/strong> - selects a small subset from the overall corpus which might be relevant to the user.&lt;/li>
&lt;li>&lt;strong>Ranking&lt;/strong> - ranks the candidate videos based on their relative importance.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>For Candidate Generation, the objective is to predict the next video watch. User search/watch history, demographics, etc are used by a simple feed-forward network as embeddings which are jointly learned during the training.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For Ranking, the objective is to model an expected watch time. A score is assigned based on the expected watch time and videos are sorted accordingly.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Similar neural network architecture is used for both procedures.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Offline metrics like precision, recall, ranking loss, etc. are used during development. A/B test is used to determine the final effectiveness of the model. We&amp;rsquo;ve already explored the &lt;a href="https://saikatkumardey.com/predictive-model-performance-offline-and-online-evaluations">discrepancies between offline vs online evaluation&lt;/a> in a different post.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;h3 id="candidate-generation">Candidate generation&lt;/h3>
&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1630224116536/RukzfmlR1.png" alt="Screenshot 2021-08-29 at 1.31.51 PM.png">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>** Problem Formulation:** Recommendation is formulated as an &lt;em>extreme multi-class classification&lt;/em>
P(w_t = i | U,C) = softmax(v_i . u)
where,
w_t = video v_i watched at time t
U = user
C = context
v_i = dense video embeddings
u = dense user embeddings&lt;/p>
&lt;ul>
&lt;li>The task of the deep neural network is to learn the embeddings &lt;em>u&lt;/em> as a function of user history and context.&lt;/li>
&lt;li>user completing a video watch is a positive example.&lt;/li>
&lt;li>&lt;a href="https://www.tensorflow.org/extras/candidate_sampling.pdf">candidate sampling&lt;/a> &amp;amp; &lt;a href="https://en.wikipedia.org/wiki/Importance_sampling">importance weighting&lt;/a> is used to sample negative examples.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Embeddings describing the watch history, user query, demographics, etc are fed to a simple feed-forward neural network having a final softmax layer to learn the class probabilities.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>watch history&lt;/strong>: dense vector representation of watched video is learned from a sequence of video-ids (just like word2vec).&lt;/li>
&lt;li>&lt;strong>user query&lt;/strong>: n-gram representations&lt;/li>
&lt;li>&lt;strong>demographics&lt;/strong>: geographic region, device, age, etc. are used as numerical/categorical features&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>The embeddings are jointly learned while training the model via gradient descent back-propagation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Age of the video is used to model the time-dependent nature of popular videos. Otherwise, the good old popular videos are going to be selected most of the times, isn&amp;rsquo;t it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>What&amp;rsquo;s interesting is that a lot of features were &amp;ldquo;engineered&amp;rdquo; as opposed to the promise of deep learning to reduce it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Training data &amp;amp; label:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>There&amp;rsquo;s an inherent sequence of video consumption. Hence, using random held-out data will be cheating since future information will leak into the training process. The model will overfit! Think about time-series forecasting. A random train-test split won&amp;rsquo;t work since the future data is not available in production during serving time.
&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1630225522918/zoyaMC-Ae.png" alt="Screenshot 2021-08-29 at 1.55.18 PM.png">&lt;/li>
&lt;li>The authors propose a model of predicting &lt;em>user&amp;rsquo;s next watch&lt;/em> instead of a randomly held-out watch. This makes sense, as we consume videos in a sequence. For example, if you&amp;rsquo;re watching a series with several episodes, recommending a random episode from that series doesn&amp;rsquo;t make sense.
&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1630225542842/UAGeaWt8C.png" alt="Screenshot 2021-08-29 at 1.55.39 PM.png">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Serving:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>To score millions of videos in latency of tens of milliseconds, a nearest neighbor-based search algorithm is used. Exact probability values of softmax() are not required. Hence, a dot product of user and video embeddings could be used to figure out the propensity score of a user &lt;em>u&lt;/em> for a particular video &lt;em>v_i&lt;/em>. A nearest neighbor search algorithm could be used to figure out the top K candidate videos based on the score.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="ranking">Ranking&lt;/h3>
&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1630229225282/zDkuTAg45.png" alt="Screenshot 2021-08-29 at 2.56.54 PM.png">&lt;/p>
&lt;ul>
&lt;li>Candidate generation selects a few hundred out of millions of videos. The ranking procedure could make use of more video features as well as user&amp;rsquo;s interactions with it in order to figure out an order of recommendation.&lt;/li>
&lt;li>The model architecture is similar to the candidate generation procedure. We assign a score to the videos using weighted logistic regression.&lt;/li>
&lt;li>The objective to optimize is a function of expected watch time per impression.&lt;/li>
&lt;li>Why not click-through rate? Well, that would promote clickbait videos instead of quality content. Watch time is a better signal that captures engagement.&lt;/li>
&lt;li>&lt;strong>Modeling Expected Watch Time&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Objective&lt;/strong>: Predict expected watch time for a given video.&lt;/li>
&lt;li>&lt;strong>Model&lt;/strong>: Weighted Logistic Regression, since the class distributions are imbalanced.
&lt;ul>
&lt;li>&lt;strong>Positive example&lt;/strong>: the video was watched.&lt;/li>
&lt;li>&lt;strong>Negative example&lt;/strong>: the video was not clicked.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>**What are the weights in &amp;ldquo;weighted&amp;rdquo; logistic regression? **
&lt;ul>
&lt;li>Positive examples are weighted by the watch time.&lt;/li>
&lt;li>Negative examples are given a weight of 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Loss&lt;/strong>: cross-entropy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf">Deep Neural Networks for YouTube Recommendations&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Predictive Model Performance Online and Offline Eval</title><link>https://saikatkumardey.com/essays/predictive-model-performance-online-and-offline-eval/</link><pubDate>Wed, 25 Aug 2021 09:57:00 +0530</pubDate><guid>https://saikatkumardey.com/essays/predictive-model-performance-online-and-offline-eval/</guid><description>&lt;p>Evaluation is an important topic in every machine learning project. There are offline metrics that we compute on the historical data. It&amp;rsquo;s supposed to provide us an indication of our model performance on real data. However, we often see a discrepancy in offline vs online performance.&lt;/p>
&lt;p>In &lt;a href="https://dl.acm.org/doi/abs/10.1145/2487575.2488215">Predictive model performance: Offline and online evaluations&lt;/a>, the authors investigate the offline/online model performance on advertisement data from Bing Search Engine.&lt;/p>
&lt;h2 id="key-takeaways">Key takeaways&lt;/h2>
&lt;ul>
&lt;li>Evaluation metrics are important since it guides what the model optimizes on. Tuning on incorrect metrics might provide misleading results in offline settings that might surprise us in production.&lt;/li>
&lt;li>AUC is a really good metric to determine model classification efficiency.&lt;/li>
&lt;li>Offline evaluation using AUC doesn&amp;rsquo;t correlate to the online evaluation via A/B tests.&lt;/li>
&lt;li>The authors propose the usage of a simulation metric that simulates user behavior based on historical logs, which works better in the online evaluation.&lt;/li>
&lt;/ul>
&lt;h2 id="details">Details&lt;/h2>
&lt;p>&lt;strong>What is an offline evaluation?&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>In typical ML projects, we split our dataset into train/test sets.&lt;/li>
&lt;li>Models are trained on the train set.&lt;/li>
&lt;li>Evaluation is done on the test set.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What is an online evaluation?&lt;/strong>&lt;/p>
&lt;p>When our model is in production, we perform an A/B test.
Typically, it has 2 variants.&lt;/p>
&lt;ul>
&lt;li>&lt;em>control&lt;/em> group with our existing model&lt;/li>
&lt;li>&lt;em>test&lt;/em> group with the new model.
Live traffic is split into the two groups &amp;amp; metrics like conversion rate, revenue per visitor, etc are measured. If the new model&amp;rsquo;s performance is statistically significant, it&amp;rsquo;s selected for launch.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>The issue?&lt;/strong>&lt;/p>
&lt;p>Offline performance doesn&amp;rsquo;t always correlate to online performance due to the dynamic nature of the latter.&lt;/p>
&lt;h2 id="evaluation-metrics">Evaluation Metrics&lt;/h2>
&lt;ul>
&lt;li>The paper focuses on metrics used for click prediction problems that most search engines like Google, Bing, etc. face.&lt;/li>
&lt;li>Click prediction problems estimates the CTRs of ads given a query.&lt;/li>
&lt;li>This is treated as a binary classification problem.&lt;/li>
&lt;/ul>
&lt;p>We&amp;rsquo;ll review some important evaluation metrics for the use case.&lt;/p>
&lt;h3 id="auc-area-under-curve">AUC (Area under curve)&lt;/h3>
&lt;ul>
&lt;li>Let&amp;rsquo;s say that we have a binary classifier that predicts a probability &lt;em>p&lt;/em> for an event to occur. Then, &lt;em>1-p&lt;/em> is the probability that the event doesn&amp;rsquo;t occur. We need a threshold to determine the class membership. AUC provides a single score that tells us how good a model is across all possible ranges of thresholds.&lt;/li>
&lt;li>AUC is computed from a ROC (Receiver Operating Characteristics) curve.&lt;/li>
&lt;li>ROC curve = a graphical representation of TPR (true positive rate) as a function of FPR (false positive rate) of a binary classifier across different thresholds.&lt;/li>
&lt;/ul>
&lt;h3 id="rig-relative-information-gain">RIG (Relative Information Gain)&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>RIG = 1 - log_loss/entropy(y)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>where,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>log_loss = - [ c*log(p) + (1-c)*log(1-p) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>entropy(y) = - [ y*logy + (1-y)*log(1-y) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>c = observed click
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>p = probability of a click
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y = CTR
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Higher is better.&lt;/p>
&lt;h3 id="prediction-error-pe">Prediction Error (PE)&lt;/h3>
&lt;p>&lt;code>PE = avg(p)/y - 1&lt;/code>&lt;/p>
&lt;ul>
&lt;li>PE = 0 when average(p) exactly matches the click-through rate.&lt;/li>
&lt;li>It could also be 0 if there&amp;rsquo;s a mix of over-estimation/under-estimation of the CTR as long the average is closer to CTR.&lt;/li>
&lt;li>It&amp;rsquo;s not a reliable metric.&lt;/li>
&lt;/ul>
&lt;h3 id="simulation-metric">Simulation Metric&lt;/h3>
&lt;p>This part is really important. It teaches us a way to simulate different model performances offline without having to run expensive A/B tests.&lt;/p>
&lt;ul>
&lt;li>A/B tests run with a fixed set of model parameters.
&lt;ul>
&lt;li>It could be expensive to run multiple experiments with different model parameters.&lt;/li>
&lt;li>It could also ruin the user experience, make losses if the new model underperforms&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The paper proposes a simulation of the user behavior offline aka auction simulation.&lt;/li>
&lt;li>Auction simulation reruns ad auctions offline for a given query and selects a set of ads based on the new model prediction scores.&lt;/li>
&lt;li>user clicks are estimated in the following way:
&lt;ul>
&lt;li>if (user, ad) pair is found in the logs
&lt;ul>
&lt;li>if it&amp;rsquo;s in the same position in history as in the simulation, use the historic CTR directly as the expected CTR&lt;/li>
&lt;li>if it&amp;rsquo;s not in the same position, the expected CTR is calibrated based on the position.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>if (user, ad) pair is not found, average CTR is used as the expected CTR.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="issues">Issues&lt;/h2>
&lt;h3 id="auc">AUC&lt;/h3>
&lt;ul>
&lt;li>ignores predicted probability values. It&amp;rsquo;s insensitive to the ranking based on the probability score. It&amp;rsquo;s possible to have different rankings with similar AUC scores.&lt;/li>
&lt;li>summarizes the test performance over the entire range of the ROC space, even where one would rarely operate on. Higher ROC doesn&amp;rsquo;t mean a better ranking.&lt;/li>
&lt;li>It weights false-positive and false negatives equally. In real life, the cost of not showing a relevant ad (false negatives) is way more than showing a sub-optimal ad (false positive).&lt;/li>
&lt;li>highly dependent on the underlying data distribution.&lt;/li>
&lt;/ul>
&lt;h3 id="rig">RIG&lt;/h3>
&lt;ul>
&lt;li>Highly sensitive to underlying data distribution.&lt;/li>
&lt;li>We can&amp;rsquo;t judge a model by just using RIG alone.&lt;/li>
&lt;li>We could compare the relative performance of different models trained/tested on the same data.&lt;/li>
&lt;/ul>
&lt;h2 id="offline-vs-online-discrepancy">Offline vs Online discrepancy&lt;/h2>
&lt;p>The authors compare 2 models&lt;/p>
&lt;ul>
&lt;li>model 1 (baseline): tuned on offline metrics like AUC &amp;amp; RIG&lt;/li>
&lt;li>model 2 (test): tuned on the simulation metric&lt;/li>
&lt;/ul>
&lt;p>The finding: model performs well on offline metrics but has a significant dip on online metrics.&lt;/p>
&lt;p>&lt;strong>Why do we see this?&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1629743092351/g_2-BgJ8G.png" alt="Screenshot 2021-08-23 at 11.54.47 PM.png">&lt;/p>
&lt;ul>
&lt;li>Tuning a model on offline metrics like AUC/RIG over-estimates the probability scores at the lower end of the score range.&lt;/li>
&lt;li>Over-estimation of the probability score at the higher end of the score range doesn&amp;rsquo;t matter much since they&amp;rsquo;ll be selected by either model.&lt;/li>
&lt;li>Over-estimation at the lower end of the score range is bad since irrelevant ads are more likely to be shown in that case.&lt;/li>
&lt;li>Offline metrics like AUC/RIG provide an overall score based on the entire range of probability scores - they&amp;rsquo;re not able to capture the intended effect.&lt;/li>
&lt;li>Tuning a model based on the simulation metric correlates better with online performance tests via A/B tests.&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2>
&lt;p>&lt;a href="https://dl.acm.org/doi/abs/10.1145/2487575.2488215">Predictive Model Performance: Offline and Online Evaluations&lt;/a>&lt;/p></description></item></channel></rss>