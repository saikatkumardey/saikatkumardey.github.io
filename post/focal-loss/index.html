<!doctype html><html dir=ltr lang=en data-theme class="html theme--light"><head><title>Solving Class Imbalance with Focal Loss |
Saikat Kumar Dey</title><meta charset=utf-8><meta name=generator content="Hugo 0.111.3"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Saikat Kumar Dey"><meta name=description content="Machine Learning Engineer"><link rel=stylesheet href=/scss/main.min.1147aa5bacb4bce677a0e264073829caedb82fd18ea07a5f1d80521f539d1c45.css integrity="sha256-EUeqW6y0vOZ3oOJkBzgpyu24L9GOoHpfHYBSH1OdHEU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicons/favicon-16x16.png><link rel=canonical href=https://saikatkumardey.com/post/focal-loss/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://saikatkumardey.com/images/site-feature-image.png"><meta name=twitter:title content="Solving Class Imbalance with Focal Loss"><meta name=twitter:description content="Focal loss is an effective technique for addressing class imbalance in machine learning by weighting the loss function to focus on hard-to-classify examples."><meta property="og:title" content="Solving Class Imbalance with Focal Loss"><meta property="og:description" content="Focal loss is an effective technique for addressing class imbalance in machine learning by weighting the loss function to focus on hard-to-classify examples."><meta property="og:type" content="article"><meta property="og:url" content="https://saikatkumardey.com/post/focal-loss/"><meta property="og:image" content="https://saikatkumardey.com/images/site-feature-image.png"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-12-01T20:49:34+05:30"><meta property="article:modified_time" content="2022-12-01T20:49:34+05:30"><meta property="og:site_name" content="Saikat Kumar Dey"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"Solving Class Imbalance with Focal Loss","headline":"Solving Class Imbalance with Focal Loss","alternativeHeadline":"","description":"
      
        Focal loss is an effective technique for addressing class imbalance in machine learning by weighting the loss function to focus on hard-to-classify examples.


      


    ","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/saikatkumardey.com\/post\/focal-loss\/"},"author":{"@type":"Person","name":"Saikat Kumar Dey"},"creator":{"@type":"Person","name":"Saikat Kumar Dey"},"accountablePerson":{"@type":"Person","name":"Saikat Kumar Dey"},"copyrightHolder":{"@type":"Person","name":"Saikat Kumar Dey"},"copyrightYear":"2022","dateCreated":"2022-12-01T20:49:34.00Z","datePublished":"2022-12-01T20:49:34.00Z","dateModified":"2022-12-01T20:49:34.00Z","publisher":{"@type":"Organization","name":"Saikat Kumar Dey","url":"https://saikatkumardey.com","logo":{"@type":"ImageObject","url":"https:\/\/saikatkumardey.com\/favicons\/favicon-32x32.png","width":"32","height":"32"}},"image":["https://saikatkumardey.com/images/site-feature-image.png"],"url":"https:\/\/saikatkumardey.com\/post\/focal-loss\/","wordCount":"1378","genre":["essay"],"keywords":[]}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/images/dp.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/>Saikat Kumar Dey</a></div><div class=sidebar__introduction-description><p>Machine Learning Engineer</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://linkedin.com/in/saikatkumardey target=_blank rel="noopener me" aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/saikatkumardey/ target=_blank rel="noopener me" aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://www.instagram.com/saikatkumardey/ target=_blank rel="noopener me" aria-label=instagram title=instagram><i class="fab fa-instagram fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://twitter.com/saikatkrdey target=_blank rel="noopener me" aria-label=twitter title=twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Saikat Kumar Dey
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Home</a></li><li class=nav__list-item><a href=/post/ title>Posts</a></li><li class=nav__list-item><a href=/projects/ title>Projects</a></li><li class=nav__list-item><a href=/now/ title>Now</a></li><li class=nav__list-item><a href=/contact/ title>Contact</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Solving Class Imbalance With Focal Loss</h1><ul class=post__meta><li class=post__meta-item><em class="fas fa-calendar-day post__meta-icon"></em>
<span class=post__meta-text>1/12/2022</span></li><li class=post__meta-item><em class="fas fa-stopwatch post__meta-icon"></em>
<span class=post__meta-text>7-minute read</span></li></ul><p>Class imbalance occurs when the number of observations in one or more classes is significantly different from the number of observations in other classes. This can lead to poor model performance, particularly when the minority class is important to predict accurately.</p><p>Focal loss is a loss function that down-weights the contribution of well-classified examples and focuses on hard-to-classify examples. This can help to improve the performance of a machine learning model when dealing with class imbalance.</p><p>In this post, we will understand the concept of focal loss and how it can be implemented in a machine learning model. We will also compare its effectiveness to other methods for solving class imbalance.</p><h2 id=focal-loss>Focal loss</h2><p>Focal loss is a variant of the cross-entropy loss function that is specifically designed to address class imbalance. It is defined as:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>FL(p) = -alpha * (1-p)^gamma * log(p)
</span></span></code></pre></div><p>where</p><ul><li>p is the predicted probability of the correct class</li><li>alpha is a weighting factor for each sample,</li><li>gamma is a tunable focusing parameter.</li></ul><p>The focal loss function has the effect of down-weighting well-classified examples, and up-weighting examples that are misclassified. This can be useful in class imbalance scenarios, where the model may be inclined to simply predict the majority class for all examples in order to achieve a high overall accuracy. By using focal loss, the model is encouraged to focus on the hard examples, and to learn to predict the minority class more accurately.</p><h2 id=focal-loss-parameters---alpha-and-gamma>Focal loss parameters - alpha and gamma</h2><p>In the focal loss formula, alpha and gamma are two tunable parameters that control the behavior of the loss function.</p><p><code>alpha</code> is a weighting factor that is applied to each sample in the batch. It has the effect of down-weighting well-classified examples and up-weighting examples that are misclassified. The value of alpha is typically set such that the majority class has a lower weighting than the minority class, to help the model focus on the hard examples.</p><p><code>gamma</code> is a focusing parameter that controls the degree to which the loss is down-weighted for well-classified examples. The value of gamma is typically set to a value greater than 1, which has the effect of down-weighting the loss for well-classified examples more heavily. This can help the model to focus on the hard examples and to learn to predict the minority class more accurately.</p><h2 id=code-sample>Code sample</h2><p>Here is an example of focal loss implemented in PyTorch:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>focal_loss</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>targets</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>gamma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>logits</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>weights</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>probs</span><span class=p>)</span> <span class=o>**</span> <span class=n>gamma</span>
</span></span><span class=line><span class=cl>    <span class=n>focal_loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>weights</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>probs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>focal_loss</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span></code></pre></div><p>This function takes the model&rsquo;s predicted logits, the true targets, and the values of <code>alpha</code> and <code>gamma</code> as input and returns the average focal loss. It can then be used in the training loop of a machine learning model as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Train the model</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Calculate predicted logits</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Calculate focal loss</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>focal_loss</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>targets</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.25</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>2.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Backpropagate and update the model weights</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span></code></pre></div><p>In this example, we use the focal loss function to calculate the loss for each batch of training data and update the model weights using backpropagation. The values of <code>alpha</code> and <code>gamma</code> can be adjusted as needed to achieve the best performance.</p><h2 id=thoughts-on-parameter-tuning>Thoughts on parameter tuning</h2><p>In the example above, the focal loss function is defined as a partial function, with alpha and gamma as fixed parameters. The model is trained using the Adam optimizer, and the focal loss is used as the criterion. By adjusting the values of <code>alpha</code> and <code>gamma</code>, you can tune the focal loss function to suit your specific dataset and task.</p><p>The values of alpha and gamma can be tuned to suit the specific dataset and task. In our example, we used <code>alpha=0.75</code> and <code>gamma=2.0</code>, but these values may not be optimal for all datasets. By experimenting with different values of these parameters, you can find the settings that produce the best results for your model.</p><p>For example, if you increase the value of alpha, the weighting of the majority class will be decreased, and the weighting of the minority class will be increased. This can help the model to focus more on the minority class, and to learn to predict it more accurately. Similarly, if you increase the value of gamma, the down-weighting of well-classified examples will be increased, which can also help the model to focus more on the hard examples.</p><h1 id=comparison-with-other-methods>Comparison with Other Methods</h1><table><thead><tr><th>Method</th><th>Description</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td>Undersampling</td><td>Removing examples from the majority class to balance the distribution of the training data</td><td>Can be effective in reducing class imbalance</td><td>Can lead to loss of important information from the majority class</td></tr><tr><td>Oversampling</td><td>Adding synthetic examples to the minority class to balance the distribution of the training data</td><td>Can be effective in reducing class imbalance</td><td>Can lead to overfitting if synthetic examples are not representative of the true data distribution</td></tr><tr><td>Focal Loss</td><td>Loss function that down-weights the contribution of well-classified examples and focuses on hard-to-classify examples</td><td>Does not require changing the distribution of the training data</td><td>Requires tuning of hyperparameters to achieve optimal performance</td></tr></tbody></table><p>As shown in the table, focal loss has several advantages over undersampling and oversampling for dealing with class imbalance. It does not require changing the distribution of the training data, and it can be easily implemented in any machine learning model. However, it does require tuning of the hyperparameters <code>alpha</code> and <code>gamma</code> to achieve optimal performance.</p><h2 id=example>Example</h2><p>In this example, we will demonstrate the effectiveness of focal loss for solving class imbalance in a machine learning model. We will use a real-world example of a binary classification problem to illustrate how focal loss can improve the performance of a model on an imbalanced dataset.</p><p>Class imbalance is a common problem in medical imaging applications where the number of positive examples (e.g. diseased images) is often much smaller than the number of negative examples (e.g. healthy images). In this case study, we will consider a dataset of medical images and a binary classification task of predicting whether an image is healthy or diseased. The dataset is imbalanced, with a large number of healthy images and a small number of diseased images.</p><p>To start, we will train a convolutional neural network (CNN) using the cross-entropy loss as the training loss. This loss function is commonly used in classification tasks, as it measures the difference between the predicted probabilities and the true labels. However, it has the disadvantage of treating all classes equally, which can result in poor performance on imbalanced datasets.</p><table><thead><tr><th>Image</th><th>True Label</th><th>Healthy</th><th>Diseased</th><th>Cross-Entropy Loss</th><th>Focal Loss</th></tr></thead><tbody><tr><td>1</td><td>Healthy</td><td>0.8</td><td>0.2</td><td>0.2231435</td><td>0.0436035</td></tr><tr><td>2</td><td>Healthy</td><td>0.9</td><td>0.1</td><td>0.1053605</td><td>0.0160707</td></tr><tr><td>3</td><td>Healthy</td><td>0.7</td><td>0.3</td><td>0.3566749</td><td>0.0479158</td></tr><tr><td>4</td><td>Healthy</td><td>0.6</td><td>0.4</td><td>0.5150763</td><td>0.0687455</td></tr><tr><td>5</td><td>Diseased</td><td>0.1</td><td>0.9</td><td>0.1053605</td><td>0.8</td></tr></tbody></table><p>As we can see, the focal loss for the diseased image is significantly higher than the loss for the healthy images. This is because the focal loss down-weights the contribution of well-classified examples and focuses on hard-to-classify examples. In this case, the healthy images are well-classified and have a low focal loss, while the diseased image is hard-to-classify and has a high focal loss. This helps the model to better learn the patterns in the minority class of diseased images.</p><h2 id=conclusion>Conclusion</h2><p>In this blog post, we explored the problem of class imbalance in machine learning and introduced focal loss as a potential solution. We discussed the motivations behind focal loss and provided an example of how to implement it in a machine learning model. We also compared focal loss to other methods for solving class imbalance, such as undersampling and oversampling. Finally, we provided a case study of using focal loss to improve the performance of a machine learning model with class imbalance.</p><p>Overall, focal loss is an effective technique for addressing class imbalance in machine learning. It can improve the performance of models by weighting the loss function to focus on hard-to-classify examples, while still allowing easy examples to contribute to the learning process.</p><p>If you&rsquo;re interested in learning more about focal loss and other methods for solving class imbalance, there are many resources available online. Some suggested readings include:</p><ul><li><a href=https://arxiv.org/abs/1708.02002>Focal Loss for Dense Object Detection</a></li><li><a href=https://hasty.ai/docs/mp-wiki/loss/focal-loss>Focal Loss</a></li><li><a href=https://www.analyticsvidhya.com/blog/2020/08/a-beginners-guide-to-focal-loss-in-object-detection/>A Beginner&rsquo;s Guide To Focal Loss In Object Detection</a></li></ul></div><div class=post__footer><span><a class=category href=/categories/essay/>essay</a></span></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Saikat Kumar Dey
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></body></html>