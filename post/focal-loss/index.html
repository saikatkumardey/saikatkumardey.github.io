<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Solving Class Imbalance with Focal Loss - Saikat Kumar Dey</title><meta name=description content="Focal loss is an effective technique for addressing class imbalance in machine learning by weighting the loss function to focus on hard-to-classify examples."><meta name=author content="Saikat Kumar Dey"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"Saikat Kumar Dey","url":"https:\/\/saikatkumardey.com"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Organization","name":"","url":"https:\/\/saikatkumardey.com"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https:\/\/saikatkumardey.com","name":"home"}},{"@type":"ListItem","position":3,"item":{"@id":"https:\/\/saikatkumardey.com\/post\/focal-loss\/","name":"Solving class imbalance with focal loss"}}]}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","author":{"name":"Saikat Kumar Dey"},"headline":"Solving Class Imbalance with Focal Loss","description":"Focal loss is an effective technique for addressing class imbalance in machine learning by weighting the loss function to focus on hard-to-classify examples.","inLanguage":"en","wordCount":1378,"datePublished":"2022-12-01T20:49:34","dateModified":"2022-12-01T20:49:34","image":"https:\/\/saikatkumardey.com","keywords":[""],"mainEntityOfPage":"https:\/\/saikatkumardey.com\/post\/focal-loss\/","publisher":{"@type":"Organization","name":"https:\/\/saikatkumardey.com","logo":{"@type":"ImageObject","url":"https:\/\/saikatkumardey.com","height":60,"width":60}}}</script><meta property="og:url" content="https://saikatkumardey.com/post/focal-loss/"><meta property="og:type" content="website"><meta property="og:site_name" content="Saikat Kumar Dey"><link rel=apple-touch-icon sizes=180x180 href=https://saikatkumardey.com/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://saikatkumardey.com/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://saikatkumardey.com/favicon/favicon-16x16.png><meta name=generator content="Hugo 0.115.4"><link rel=alternate href=https://saikatkumardey.com/index.xml type=application/rss+xml title="Saikat Kumar Dey"><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-KK94CHFLLe+nY2dmCWGMq91rCGa5gtU4mk92HdvYe+M/SXH301p5ILy+dN9+nJOZ crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css><link rel=stylesheet href=https://saikatkumardey.com/css/main.css><link disabled id=dark-mode-theme rel=stylesheet href=https://saikatkumardey.com/css/dark.css><link rel=stylesheet href=https://saikatkumardey.com/css/highlight.min.css><link rel=stylesheet href=https://saikatkumardey.com/css/codeblock.css></head><body><div class="container fixed-top"><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-10 col-xl-10"><nav class="navbar navbar-expand-lg navbar-light fixed-top p-0"><div class=container><a class="navbar-brand fw-bold" href=https://saikatkumardey.com>Saikat Kumar Dey</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse justify-content-end" id=navbarNav><ul class="navbar-nav mb-2 mb-lg-0"><li class=nav-item><a class=nav-link title=Home href=/>Home</a></li><li class=nav-item><a class=nav-link title=Projects href=/projects/>Projects</a></li><li class=nav-item><a class=nav-link title=Now href=/now/>Now</a></li><li class="nav-item nav-link"><a id=dark-mode-toggle class="bi bi-sun"></a></li></ul></div></div></nav></div></div></div><header class=header-section><div class="intro-header no-img mt-10"><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-8 col-xl-8"><div class="col-sm-12 col-md-12 col-lg-12 col-xl-12"><div class=post-heading><h1 class="fw-semibold display-5 lh-1 mb-3">Solving Class Imbalance with Focal Loss</h1><span class=post-meta>&nbsp;December 1, 2022</span></div></div></div></div></div></div></header><div class=container role=main><div class=row><div class="col-lg-8 offset-lg-2 col-md-10 offset-md-1"><div class="card-image card-image-blog p-0"></div></div><div class="col-lg-8 offset-lg-2 col-md-10 offset-md-1 pt-4"><article role=main class=blog-post><p>Class imbalance occurs when the number of observations in one or more classes is significantly different from the number of observations in other classes. This can lead to poor model performance, particularly when the minority class is important to predict accurately.</p><p>Focal loss is a loss function that down-weights the contribution of well-classified examples and focuses on hard-to-classify examples. This can help to improve the performance of a machine learning model when dealing with class imbalance.</p><p>In this post, we will understand the concept of focal loss and how it can be implemented in a machine learning model. We will also compare its effectiveness to other methods for solving class imbalance.</p><h2 id=focal-loss>Focal loss</h2><p>Focal loss is a variant of the cross-entropy loss function that is specifically designed to address class imbalance. It is defined as:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>FL(p) = -alpha * (1-p)^gamma * log(p)
</span></span></code></pre></div><p>where</p><ul><li>p is the predicted probability of the correct class</li><li>alpha is a weighting factor for each sample,</li><li>gamma is a tunable focusing parameter.</li></ul><p>The focal loss function has the effect of down-weighting well-classified examples, and up-weighting examples that are misclassified. This can be useful in class imbalance scenarios, where the model may be inclined to simply predict the majority class for all examples in order to achieve a high overall accuracy. By using focal loss, the model is encouraged to focus on the hard examples, and to learn to predict the minority class more accurately.</p><h2 id=focal-loss-parameters---alpha-and-gamma>Focal loss parameters - alpha and gamma</h2><p>In the focal loss formula, alpha and gamma are two tunable parameters that control the behavior of the loss function.</p><p><code>alpha</code> is a weighting factor that is applied to each sample in the batch. It has the effect of down-weighting well-classified examples and up-weighting examples that are misclassified. The value of alpha is typically set such that the majority class has a lower weighting than the minority class, to help the model focus on the hard examples.</p><p><code>gamma</code> is a focusing parameter that controls the degree to which the loss is down-weighted for well-classified examples. The value of gamma is typically set to a value greater than 1, which has the effect of down-weighting the loss for well-classified examples more heavily. This can help the model to focus on the hard examples and to learn to predict the minority class more accurately.</p><h2 id=code-sample>Code sample</h2><p>Here is an example of focal loss implemented in PyTorch:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>torch</span>
</span></span><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>focal_loss</span>(logits, targets, alpha, gamma):
</span></span><span style=display:flex><span>    probs <span style=font-weight:700>=</span> torch<span style=font-weight:700>.</span>sigmoid(logits)
</span></span><span style=display:flex><span>    weights <span style=font-weight:700>=</span> alpha <span style=font-weight:700>*</span> (<span style=color:#099>1</span> <span style=font-weight:700>-</span> probs) <span style=font-weight:700>**</span> gamma
</span></span><span style=display:flex><span>    focal_loss <span style=font-weight:700>=</span> <span style=font-weight:700>-</span>weights <span style=font-weight:700>*</span> torch<span style=font-weight:700>.</span>log(probs)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> focal_loss<span style=font-weight:700>.</span>mean()
</span></span></code></pre></div><p>This function takes the model&rsquo;s predicted logits, the true targets, and the values of <code>alpha</code> and <code>gamma</code> as input and returns the average focal loss. It can then be used in the training loop of a machine learning model as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Train the model</span>
</span></span><span style=display:flex><span><span style=font-weight:700>for</span> inputs, targets <span style=font-weight:700>in</span> train_loader:
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Calculate predicted logits</span>
</span></span><span style=display:flex><span>    logits <span style=font-weight:700>=</span> model(inputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Calculate focal loss</span>
</span></span><span style=display:flex><span>    loss <span style=font-weight:700>=</span> focal_loss(logits, targets, alpha<span style=font-weight:700>=</span><span style=color:#099>0.25</span>, gamma<span style=font-weight:700>=</span><span style=color:#099>2.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Backpropagate and update the model weights</span>
</span></span><span style=display:flex><span>    loss<span style=font-weight:700>.</span>backward()
</span></span><span style=display:flex><span>    optimizer<span style=font-weight:700>.</span>step()
</span></span></code></pre></div><p>In this example, we use the focal loss function to calculate the loss for each batch of training data and update the model weights using backpropagation. The values of <code>alpha</code> and <code>gamma</code> can be adjusted as needed to achieve the best performance.</p><h2 id=thoughts-on-parameter-tuning>Thoughts on parameter tuning</h2><p>In the example above, the focal loss function is defined as a partial function, with alpha and gamma as fixed parameters. The model is trained using the Adam optimizer, and the focal loss is used as the criterion. By adjusting the values of <code>alpha</code> and <code>gamma</code>, you can tune the focal loss function to suit your specific dataset and task.</p><p>The values of alpha and gamma can be tuned to suit the specific dataset and task. In our example, we used <code>alpha=0.75</code> and <code>gamma=2.0</code>, but these values may not be optimal for all datasets. By experimenting with different values of these parameters, you can find the settings that produce the best results for your model.</p><p>For example, if you increase the value of alpha, the weighting of the majority class will be decreased, and the weighting of the minority class will be increased. This can help the model to focus more on the minority class, and to learn to predict it more accurately. Similarly, if you increase the value of gamma, the down-weighting of well-classified examples will be increased, which can also help the model to focus more on the hard examples.</p><h1 id=comparison-with-other-methods>Comparison with Other Methods</h1><table><thead><tr><th>Method</th><th>Description</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td>Undersampling</td><td>Removing examples from the majority class to balance the distribution of the training data</td><td>Can be effective in reducing class imbalance</td><td>Can lead to loss of important information from the majority class</td></tr><tr><td>Oversampling</td><td>Adding synthetic examples to the minority class to balance the distribution of the training data</td><td>Can be effective in reducing class imbalance</td><td>Can lead to overfitting if synthetic examples are not representative of the true data distribution</td></tr><tr><td>Focal Loss</td><td>Loss function that down-weights the contribution of well-classified examples and focuses on hard-to-classify examples</td><td>Does not require changing the distribution of the training data</td><td>Requires tuning of hyperparameters to achieve optimal performance</td></tr></tbody></table><p>As shown in the table, focal loss has several advantages over undersampling and oversampling for dealing with class imbalance. It does not require changing the distribution of the training data, and it can be easily implemented in any machine learning model. However, it does require tuning of the hyperparameters <code>alpha</code> and <code>gamma</code> to achieve optimal performance.</p><h2 id=example>Example</h2><p>In this example, we will demonstrate the effectiveness of focal loss for solving class imbalance in a machine learning model. We will use a real-world example of a binary classification problem to illustrate how focal loss can improve the performance of a model on an imbalanced dataset.</p><p>Class imbalance is a common problem in medical imaging applications where the number of positive examples (e.g. diseased images) is often much smaller than the number of negative examples (e.g. healthy images). In this case study, we will consider a dataset of medical images and a binary classification task of predicting whether an image is healthy or diseased. The dataset is imbalanced, with a large number of healthy images and a small number of diseased images.</p><p>To start, we will train a convolutional neural network (CNN) using the cross-entropy loss as the training loss. This loss function is commonly used in classification tasks, as it measures the difference between the predicted probabilities and the true labels. However, it has the disadvantage of treating all classes equally, which can result in poor performance on imbalanced datasets.</p><table><thead><tr><th>Image</th><th>True Label</th><th>Healthy</th><th>Diseased</th><th>Cross-Entropy Loss</th><th>Focal Loss</th></tr></thead><tbody><tr><td>1</td><td>Healthy</td><td>0.8</td><td>0.2</td><td>0.2231435</td><td>0.0436035</td></tr><tr><td>2</td><td>Healthy</td><td>0.9</td><td>0.1</td><td>0.1053605</td><td>0.0160707</td></tr><tr><td>3</td><td>Healthy</td><td>0.7</td><td>0.3</td><td>0.3566749</td><td>0.0479158</td></tr><tr><td>4</td><td>Healthy</td><td>0.6</td><td>0.4</td><td>0.5150763</td><td>0.0687455</td></tr><tr><td>5</td><td>Diseased</td><td>0.1</td><td>0.9</td><td>0.1053605</td><td>0.8</td></tr></tbody></table><p>As we can see, the focal loss for the diseased image is significantly higher than the loss for the healthy images. This is because the focal loss down-weights the contribution of well-classified examples and focuses on hard-to-classify examples. In this case, the healthy images are well-classified and have a low focal loss, while the diseased image is hard-to-classify and has a high focal loss. This helps the model to better learn the patterns in the minority class of diseased images.</p><h2 id=conclusion>Conclusion</h2><p>In this blog post, we explored the problem of class imbalance in machine learning and introduced focal loss as a potential solution. We discussed the motivations behind focal loss and provided an example of how to implement it in a machine learning model. We also compared focal loss to other methods for solving class imbalance, such as undersampling and oversampling. Finally, we provided a case study of using focal loss to improve the performance of a machine learning model with class imbalance.</p><p>Overall, focal loss is an effective technique for addressing class imbalance in machine learning. It can improve the performance of models by weighting the loss function to focus on hard-to-classify examples, while still allowing easy examples to contribute to the learning process.</p><p>If you&rsquo;re interested in learning more about focal loss and other methods for solving class imbalance, there are many resources available online. Some suggested readings include:</p><ul><li><a href=https://arxiv.org/abs/1708.02002>Focal Loss for Dense Object Detection</a></li><li><a href=https://hasty.ai/docs/mp-wiki/loss/focal-loss>Focal Loss</a></li><li><a href=https://www.analyticsvidhya.com/blog/2020/08/a-beginners-guide-to-focal-loss-in-object-detection/>A Beginner&rsquo;s Guide To Focal Loss In Object Detection</a></li></ul></article></div></div><div class=row><div class="col-lg-8 offset-lg-2 col-md-10 offset-md-1"><hr></div><div class="col-lg-8 offset-lg-2 col-md-10 offset-md-1 pt-2"></div><div class="col-lg-8 offset-lg-2 col-md-10 offset-md-1 pt-4"><section id=social-share><div class="list-inline footer-links"><div class=share-box aria-hidden=true><div><p class="list-inline footer-links">Use the share button below if you liked it.</p></div><ul class=share><li><a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fsaikatkumardey.com%2fpost%2ffocal-loss%2f" target=_blank title="Share on Facebook"><i class="bi bi-facebook facebook-color"></i></a></li><li><a href="//twitter.com/share?url=https%3a%2f%2fsaikatkumardey.com%2fpost%2ffocal-loss%2f&amp;text=Solving%20Class%20Imbalance%20with%20Focal%20Loss&amp;via=map%5bemail%3asample%40gmail.com%20facebook%3ausername%20github%3asaikatkumardey%20linkedin%3asaikatkumardey%20name%3aSaikat%20Kumar%20Dey%20twitter%3asaikatkrdey%20website%3ahttps%3a%2f%2fsaikatkumardey.com%2f%20youtube%3a%5d" target=_blank title="Share on Twitter"><i class="bi bi-twitter twitter-color"></i></a></li><li><a href="//reddit.com/submit?url=https%3a%2f%2fsaikatkumardey.com%2fpost%2ffocal-loss%2f&amp;title=Solving%20Class%20Imbalance%20with%20Focal%20Loss" target=_blank title="Share on Reddit"><i class="bi bi-reddit reddit-color"></i></a></li><li><a href="whatsapp://send?text=https%3a%2f%2fsaikatkumardey.com%2fpost%2ffocal-loss%2f&amp;description=Solving%20Class%20Imbalance%20with%20Focal%20Loss" target=_blank rel=noopener title="Share on WhatsApp"><i class="bi bi-whatsapp whatsapp-color"></i></a></li></ul></div></div></section></div><div class="col-lg-8 offset-lg-2 col-md-10 offset-md-1 pt-4"><ul class="list-group list-group-horizontal" style=flex-direction:row><li class="list-group-item b-0"><a type=button class="btn btn-dark" role=button href=https://saikatkumardey.com/post/leetcode-patterns/ data-toggle=tooltip data-placement=top title="Mastering Common Patterns to Solve Leetcode Problems">&larr;
Previous Post</a></li><li class="list-group-item ms-auto b-0"><a type=button class="btn btn-dark" role=button href=https://saikatkumardey.com/post/fbeta/ data-toggle=tooltip data-placement=top title="Beyond F1: using the Fbeta_score for better model evaluation">Next Post
&rarr;</a></li></ul></div><div class="col-lg-8 offset-lg-2 col-md-10 offset-md-1 pt-4"></div></div></div><div><div class=container><div class=row></div></div></div></div></div><footer><div class=container><div class=row><div class=col-md-12><ul class="list-inline list-group list-group-horizontal text-center footer-links d-flex justify-content-center flex-row"><li><a href=mailto:sample@gmail.com title="Email me" target=_blank><span class=mx-2><i class="bi bi-envelope"></i></span></a></li><li><a href=https://www.facebook.com/username title=Facebook target=_blank><span class=mx-2><i class="bi bi-facebook"></i></span></a></li><li><a href=https://github.com/saikatkumardey title=GitHub target=_blank><span class=mx-2><i class="bi bi-github"></i></span></a></li><li><a href=https://twitter.com/saikatkrdey title=Twitter target=_blank><span class=mx-2><i class="bi bi-twitter"></i></span></a></li><li><a href=https://linkedin.com/in/saikatkumardey title=LinkedIn target=_blank><span class=mx-2><i class="bi bi-linkedin"></i></span></a></li><li><a href=https://www.youtube.com/ title=Youtube target=_blank><span class=mx-2><i class="bi bi-youtube"></i></span></a></li></ul></div></div><div class=row><div class=col-md-12><p class="credits copyright text-muted"><a href=https://saikatkumardey.com/>Saikat Kumar Dey</a>
&nbsp;&bull;&nbsp;&copy;
2023
&nbsp;&bull;&nbsp;
<a href=https://saikatkumardey.com>Saikat Kumar Dey</a></p><p class="credits theme-by text-muted">Powered by <a href=https://gohugo.io>Hugo</a> & <a href=https://github.com/binokochumolvarghese/lightbi-hugo>Lightbi.</a>&nbsp; Made with ‚ù§ by <a href=https://binovarghese.com>Bino</a></p></div></div></div></footer><script src=https://code.jquery.com/jquery-3.7.0.slim.min.js integrity="sha256-tG5mcZUtJsZvyKAxYLVXrmjKBVLd6VpVccqz/r4ypFE=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/js/bootstrap.min.js integrity=sha384-Y4oOpwW3duJdCWv5ly8SCFYWqFDsfob/3GkgExXKV4idmbt98QcxXYs9UoXAB7BZ crossorigin=anonymous></script>
<script src=https://saikatkumardey.com/js/highlight.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><script>$(document).ready(function(){$("pre.chroma").css("padding","0")})</script><script src=https://saikatkumardey.com/js/dark-mode.js></script></body></html>