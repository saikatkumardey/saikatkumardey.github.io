<!doctype html><html><head><meta charset=utf-8><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Paper summary on how to reduce the discrepancy between offline and online evaluation of ML models"><base href=https://saikatkumardey.com/><title>Predictive Model Performance Online and Offline Eval | Saikat Kumar Dey</title><link rel=canonical href=https://saikatkumardey.com/essays/predictive-model-performance-online-and-offline-eval/><link href=https://saikatkumardey.com/css/style.css type=text/css rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Alegreya&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Fira+Mono&display=swap" rel=stylesheet><script async defer data-domain=saikatkumardey.com src=https://plausible.io/js/plausible.outbound-links.js></script><meta property="og:title" content="Predictive Model Performance Online and Offline Eval"><meta property="og:description" content="Paper summary on how to reduce the discrepancy between offline and online evaluation of ML models"><meta property="og:type" content="article"><meta property="og:url" content="https://saikatkumardey.com/essays/predictive-model-performance-online-and-offline-eval/"><meta property="article:section" content="essays"><meta property="article:published_time" content="2021-08-25T09:57:00+05:30"><meta property="article:modified_time" content="2021-08-25T09:57:00+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Predictive Model Performance Online and Offline Eval"><meta name=twitter:description content="Paper summary on how to reduce the discrepancy between offline and online evaluation of ML models"></head><body><div class=container><header><h1><a href=https://saikatkumardey.com/>Saikat Kumar Dey</a></h1><nav><ul><li><a href=https://saikatkumardey.com/essays/ title>Essays</a></li><li><a href=https://saikatkumardey.com/daily/ title>Daily</a></li></ul></nav></header><div id=content><main class=essay><h1 id=title>Predictive Model Performance Online and Offline Eval</h1><aside class=meta><div><span class=date>August 25, 2021 â€¢ ~1000 words</span><ul class=categories><li><i>in</i></li><li><a href=https://saikatkumardey.com/categories/research/>research</a></li><li><a href=https://saikatkumardey.com/categories/ml/>ml</a></li></ul></div></aside><div><article id=content><p>Evaluation is an important topic in every machine learning project. There are offline metrics that we compute on the historical data. It&rsquo;s supposed to provide us an indication of our model performance on real data. However, we often see a discrepancy in offline vs online performance.</p><p>In <a href=https://dl.acm.org/doi/abs/10.1145/2487575.2488215>Predictive model performance: Offline and online evaluations</a>, the authors investigate the offline/online model performance on advertisement data from Bing Search Engine.</p><h2 id=key-takeaways>Key takeaways</h2><ul><li>Evaluation metrics are important since it guides what the model optimizes on. Tuning on incorrect metrics might provide misleading results in offline settings that might surprise us in production.</li><li>AUC is a really good metric to determine model classification efficiency.</li><li>Offline evaluation using AUC doesn&rsquo;t correlate to the online evaluation via A/B tests.</li><li>The authors propose the usage of a simulation metric that simulates user behavior based on historical logs, which works better in the online evaluation.</li></ul><h2 id=details>Details</h2><p><strong>What is an offline evaluation?</strong></p><ul><li>In typical ML projects, we split our dataset into train/test sets.</li><li>Models are trained on the train set.</li><li>Evaluation is done on the test set.</li></ul><p><strong>What is an online evaluation?</strong></p><p>When our model is in production, we perform an A/B test.
Typically, it has 2 variants.</p><ul><li><em>control</em> group with our existing model</li><li><em>test</em> group with the new model.
Live traffic is split into the two groups & metrics like conversion rate, revenue per visitor, etc are measured. If the new model&rsquo;s performance is statistically significant, it&rsquo;s selected for launch.</li></ul><p><strong>The issue?</strong></p><p>Offline performance doesn&rsquo;t always correlate to online performance due to the dynamic nature of the latter.</p><h2 id=evaluation-metrics>Evaluation Metrics</h2><ul><li>The paper focuses on metrics used for click prediction problems that most search engines like Google, Bing, etc. face.</li><li>Click prediction problems estimates the CTRs of ads given a query.</li><li>This is treated as a binary classification problem.</li></ul><p>We&rsquo;ll review some important evaluation metrics for the use case.</p><h3 id=auc-area-under-curve>AUC (Area under curve)</h3><ul><li>Let&rsquo;s say that we have a binary classifier that predicts a probability <em>p</em> for an event to occur. Then, <em>1-p</em> is the probability that the event doesn&rsquo;t occur. We need a threshold to determine the class membership. AUC provides a single score that tells us how good a model is across all possible ranges of thresholds.</li><li>AUC is computed from a ROC (Receiver Operating Characteristics) curve.</li><li>ROC curve = a graphical representation of TPR (true positive rate) as a function of FPR (false positive rate) of a binary classifier across different thresholds.</li></ul><h3 id=rig-relative-information-gain>RIG (Relative Information Gain)</h3><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>RIG = 1 - log_loss/entropy(y)
</span></span><span style=display:flex><span>where,
</span></span><span style=display:flex><span>log_loss     = - [ c*log(p) + (1-c)*log(1-p) ]
</span></span><span style=display:flex><span>entropy(y) = - [ y*logy + (1-y)*log(1-y) ]
</span></span><span style=display:flex><span>c = observed click
</span></span><span style=display:flex><span>p = probability of a click
</span></span><span style=display:flex><span>y = CTR
</span></span></code></pre></div><p>Higher is better.</p><h3 id=prediction-error-pe>Prediction Error (PE)</h3><p><code>PE = avg(p)/y - 1</code></p><ul><li>PE = 0 when average(p) exactly matches the click-through rate.</li><li>It could also be 0 if there&rsquo;s a mix of over-estimation/under-estimation of the CTR as long the average is closer to CTR.</li><li>It&rsquo;s not a reliable metric.</li></ul><h3 id=simulation-metric>Simulation Metric</h3><p>This part is really important. It teaches us a way to simulate different model performances offline without having to run expensive A/B tests.</p><ul><li>A/B tests run with a fixed set of model parameters.<ul><li>It could be expensive to run multiple experiments with different model parameters.</li><li>It could also ruin the user experience, make losses if the new model underperforms</li></ul></li><li>The paper proposes a simulation of the user behavior offline aka auction simulation.</li><li>Auction simulation reruns ad auctions offline for a given query and selects a set of ads based on the new model prediction scores.</li><li>user clicks are estimated in the following way:<ul><li>if (user, ad) pair is found in the logs<ul><li>if it&rsquo;s in the same position in history as in the simulation, use the historic CTR directly as the expected CTR</li><li>if it&rsquo;s not in the same position, the expected CTR is calibrated based on the position.</li></ul></li><li>if (user, ad) pair is not found, average CTR is used as the expected CTR.</li></ul></li></ul><h2 id=issues>Issues</h2><h3 id=auc>AUC</h3><ul><li>ignores predicted probability values. It&rsquo;s insensitive to the ranking based on the probability score. It&rsquo;s possible to have different rankings with similar AUC scores.</li><li>summarizes the test performance over the entire range of the ROC space, even where one would rarely operate on. Higher ROC doesn&rsquo;t mean a better ranking.</li><li>It weights false-positive and false negatives equally. In real life, the cost of not showing a relevant ad (false negatives) is way more than showing a sub-optimal ad (false positive).</li><li>highly dependent on the underlying data distribution.</li></ul><h3 id=rig>RIG</h3><ul><li>Highly sensitive to underlying data distribution.</li><li>We can&rsquo;t judge a model by just using RIG alone.</li><li>We could compare the relative performance of different models trained/tested on the same data.</li></ul><h2 id=offline-vs-online-discrepancy>Offline vs Online discrepancy</h2><p>The authors compare 2 models</p><ul><li>model 1 (baseline): tuned on offline metrics like AUC & RIG</li><li>model 2 (test): tuned on the simulation metric</li></ul><p>The finding: model performs well on offline metrics but has a significant dip on online metrics.</p><p><strong>Why do we see this?</strong></p><p><img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1629743092351/g_2-BgJ8G.png alt="Screenshot 2021-08-23 at 11.54.47 PM.png"></p><ul><li>Tuning a model on offline metrics like AUC/RIG over-estimates the probability scores at the lower end of the score range.</li><li>Over-estimation of the probability score at the higher end of the score range doesn&rsquo;t matter much since they&rsquo;ll be selected by either model.</li><li>Over-estimation at the lower end of the score range is bad since irrelevant ads are more likely to be shown in that case.</li><li>Offline metrics like AUC/RIG provide an overall score based on the entire range of probability scores - they&rsquo;re not able to capture the intended effect.</li><li>Tuning a model based on the simulation metric correlates better with online performance tests via A/B tests.</li></ul><h2 id=references>References</h2><p><a href=https://dl.acm.org/doi/abs/10.1145/2487575.2488215>Predictive Model Performance: Offline and Online Evaluations</a></p></article></div></main></div><footer><div><p>&#169; 2022 Saikat Kumar Dey.</p></div></footer></div></body></html>