<!doctype html><html><head><meta charset=utf-8><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Paper summary on how to reduce the discrepancy between offline and online evaluation of ML models"><base href=https://saikatkumardey.com/><title>Predictive Model Performance Online and Offline Eval | Saikat Kumar Dey</title><link rel=canonical href=https://saikatkumardey.com/essays/predictive-model-performance-online-and-offline-eval/><link href=https://saikatkumardey.com/css/style2.css type=text/css rel=stylesheet><link href=https://saikatkumardey.com/css/style.css type=text/css rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Alegreya&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Fira+Mono&display=swap" rel=stylesheet><script async defer data-domain=saikatkumardey.com src=https://plausible.io/js/plausible.outbound-links.js></script><meta property="og:title" content="Predictive Model Performance Online and Offline Eval"><meta property="og:description" content="Paper summary on how to reduce the discrepancy between offline and online evaluation of ML models"><meta property="og:type" content="article"><meta property="og:url" content="https://saikatkumardey.com/essays/predictive-model-performance-online-and-offline-eval/"><meta property="article:section" content="essays"><meta property="article:published_time" content="2021-08-25T09:57:00+05:30"><meta property="article:modified_time" content="2021-08-25T09:57:00+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Predictive Model Performance Online and Offline Eval"><meta name=twitter:description content="Paper summary on how to reduce the discrepancy between offline and online evaluation of ML models"></head><body><div class=container><header><h1><a href=https://saikatkumardey.com/>Saikat Kumar Dey</a></h1><nav><ul><li><a href=https://saikatkumardey.com/essays/ title>Essays</a></li><li><a href=https://saikatkumardey.com/daily/ title>Daily</a></li></ul></nav></header><div id=content><main class=essay><h1 id=title>Predictive Model Performance Online and Offline Eval</h1><aside class=meta><div><span class=date>August 25, 2021 â€¢ ~1000 words</span><ul class=categories><li><i>in</i></li><li><a href=https://saikatkumardey.com/%20categories/research/>research</a></li><li><a href=https://saikatkumardey.com/%20categories/ml/>ml</a></li></ul></div></aside><div><article id=content><p>Evaluation is an important topic in every machine learning project. There are offline metrics that we compute on the historical data. It&rsquo;s supposed to provide us an indication of our model performance on real data. However, we often see a discrepancy in offline vs online performance.</p><p>In <a href=https://dl.acm.org/doi/abs/10.1145/2487575.2488215>Predictive model performance: Offline and online evaluations</a>, the authors investigate the offline/online model performance on advertisement data from Bing Search Engine.</p><h2 id=key-takeaways>Key takeaways</h2><ul><li>Evaluation metrics are important since it guides what the model optimizes on. Tuning on incorrect metrics might provide misleading results in offline settings that might surprise us in production.</li><li>AUC is a really good metric to determine model classification efficiency.</li><li>Offline evaluation using AUC doesn&rsquo;t correlate to the online evaluation via A/B tests.</li><li>The authors propose the usage of a simulation metric that simulates user behavior based on historical logs, which works better in the online evaluation.</li></ul><h2 id=details>Details</h2><p><strong>What is an offline evaluation?</strong></p><ul><li>In typical ML projects, we split our dataset into train/test sets.</li><li>Models are trained on the train set.</li><li>Evaluation is done on the test set.</li></ul><p><strong>What is an online evaluation?</strong></p><p>When our model is in production, we perform an A/B test.
Typically, it has 2 variants.</p><ul><li><em>control</em> group with our existing model</li><li><em>test</em> group with the new model.
Live traffic is split into the two groups & metrics like conversion rate, revenue per visitor, etc are measured. If the new model&rsquo;s performance is statistically significant, it&rsquo;s selected for launch.</li></ul><p><strong>The issue?</strong></p><p>Offline performance doesn&rsquo;t always correlate to online performance due to the dynamic nature of the latter.</p><h2 id=evaluation-metrics>Evaluation Metrics</h2><ul><li>The paper focuses on metrics used for click prediction problems that most search engines like Google, Bing, etc. face.</li><li>Click prediction problems estimates the CTRs of ads given a query.</li><li>This is treated as a binary classification problem.</li></ul><p>We&rsquo;ll review some important evaluation metrics for the use case.</p><h3 id=auc-area-under-curve>AUC (Area under curve)</h3><ul><li>Let&rsquo;s say that we have a binary classifier that predicts a probability <em>p</em> for an event to occur. Then, <em>1-p</em> is the probability that the event doesn&rsquo;t occur. We need a threshold to determine the class membership. AUC provides a single score that tells us how good a model is across all possible ranges of thresholds.</li><li>AUC is computed from a ROC (Receiver Operating Characteristics) curve.</li><li>ROC curve = a graphical representation of TPR (true positive rate) as a function of FPR (false positive rate) of a binary classifier across different thresholds.</li></ul><h3 id=rig-relative-information-gain>RIG (Relative Information Gain)</h3><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>RIG = 1 - log_loss/entropy(y)
</span></span><span style=display:flex><span>where,
</span></span><span style=display:flex><span>log_loss     = - [ c*log(p) + (1-c)*log(1-p) ]
</span></span><span style=display:flex><span>entropy(y) = - [ y*logy + (1-y)*log(1-y) ]
</span></span><span style=display:flex><span>c = observed click
</span></span><span style=display:flex><span>p = probability of a click
</span></span><span style=display:flex><span>y = CTR
</span></span></code></pre></div><p>Higher is better.</p><h3 id=prediction-error-pe>Prediction Error (PE)</h3><p><code>PE = avg(p)/y - 1</code></p><ul><li>PE = 0 when average(p) exactly matches the click-through rate.</li><li>It could also be 0 if there&rsquo;s a mix of over-estimation/under-estimation of the CTR as long the average is closer to CTR.</li><li>It&rsquo;s not a reliable metric.</li></ul><h3 id=simulation-metric>Simulation Metric</h3><p>This part is really important. It teaches us a way to simulate different model performances offline without having to run expensive A/B tests.</p><ul><li>A/B tests run with a fixed set of model parameters.<ul><li>It could be expensive to run multiple experiments with different model parameters.</li><li>It could also ruin the user experience, make losses if the new model underperforms</li></ul></li><li>The paper proposes a simulation of the user behavior offline aka auction simulation.</li><li>Auction simulation reruns ad auctions offline for a given query and selects a set of ads based on the new model prediction scores.</li><li>user clicks are estimated in the following way:<ul><li>if (user, ad) pair is found in the logs<ul><li>if it&rsquo;s in the same position in history as in the simulation, use the historic CTR directly as the expected CTR</li><li>if it&rsquo;s not in the same position, the expected CTR is calibrated based on the position.</li></ul></li><li>if (user, ad) pair is not found, average CTR is used as the expected CTR.</li></ul></li></ul><h2 id=issues>Issues</h2><h3 id=auc>AUC</h3><ul><li>ignores predicted probability values. It&rsquo;s insensitive to the ranking based on the probability score. It&rsquo;s possible to have different rankings with similar AUC scores.</li><li>summarizes the test performance over the entire range of the ROC space, even where one would rarely operate on. Higher ROC doesn&rsquo;t mean a better ranking.</li><li>It weights false-positive and false negatives equally. In real life, the cost of not showing a relevant ad (false negatives) is way more than showing a sub-optimal ad (false positive).</li><li>highly dependent on the underlying data distribution.</li></ul><h3 id=rig>RIG</h3><ul><li>Highly sensitive to underlying data distribution.</li><li>We can&rsquo;t judge a model by just using RIG alone.</li><li>We could compare the relative performance of different models trained/tested on the same data.</li></ul><h2 id=offline-vs-online-discrepancy>Offline vs Online discrepancy</h2><p>The authors compare 2 models</p><ul><li>model 1 (baseline): tuned on offline metrics like AUC & RIG</li><li>model 2 (test): tuned on the simulation metric</li></ul><p>The finding: model performs well on offline metrics but has a significant dip on online metrics.</p><p><strong>Why do we see this?</strong></p><p><img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1629743092351/g_2-BgJ8G.png alt="Screenshot 2021-08-23 at 11.54.47 PM.png"></p><ul><li>Tuning a model on offline metrics like AUC/RIG over-estimates the probability scores at the lower end of the score range.</li><li>Over-estimation of the probability score at the higher end of the score range doesn&rsquo;t matter much since they&rsquo;ll be selected by either model.</li><li>Over-estimation at the lower end of the score range is bad since irrelevant ads are more likely to be shown in that case.</li><li>Offline metrics like AUC/RIG provide an overall score based on the entire range of probability scores - they&rsquo;re not able to capture the intended effect.</li><li>Tuning a model based on the simulation metric correlates better with online performance tests via A/B tests.</li></ul><h2 id=references>References</h2><p><a href=https://dl.acm.org/doi/abs/10.1145/2487575.2488215>Predictive Model Performance: Offline and Online Evaluations</a></p></article></div><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?text=Paper%20summary%20on%20how%20to%20reduce%20the%20discrepancy%20between%20offline%20and%20online%20evaluation%20of%20ML%20models.&url=https%3a%2f%2fsaikatkumardey.com%2fessays%2fpredictive-model-performance-online-and-offline-eval%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5.0-4.55 2.04-4.55 4.54.0.36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3.0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35.0 12.92-6.92 12.92-12.93.0-.2.0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg></div></div></a><a class=resp-sharing-button__link href="mailto:?subject=Predictive%20Model%20Performance%20Online%20and%20Offline%20Eval&body=https%3a%2f%2fsaikatkumardey.com%2fessays%2fpredictive-model-performance-online-and-offline-eval%2f" target=_self rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 4H2C.9 4 0 4.9.0 6v12c0 1.1.9 2 2 2h20c1.1.0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17.0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1.0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08.0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fsaikatkumardey.com%2fessays%2fpredictive-model-performance-online-and-offline-eval%2f&title=Predictive%20Model%20Performance%20Online%20and%20Offline%20Eval&summary=Paper%20summary%20on%20how%20to%20reduce%20the%20discrepancy%20between%20offline%20and%20online%20evaluation%20of%20ML%20models&source=https%3a%2f%2fsaikatkumardey.com%2fessays%2fpredictive-model-performance-online-and-offline-eval%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6.0 2.5 1 2.6 2.5.0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/></svg></div></div></a><a class=resp-sharing-button__link href="whatsapp://send?text=Predictive%20Model%20Performance%20Online%20and%20Offline%20Eval https%3a%2f%2fsaikatkumardey.com%2fessays%2fpredictive-model-performance-online-and-offline-eval%2f" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3.0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7.0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1.0-5.2 4.2-9.4 9.4-9.4 2.5.0 4.9 1 6.7 2.8s2.8 4.2 2.8 6.7c-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2s0-.5.1-.6.3-.3.4-.5c.2-.1.3-.3.4-.5s0-.4.0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z"/></svg></div></div></a></main></div><footer><div><p>&#169; 2022 Saikat Kumar Dey.</p></div></footer></div></body></html>