<!doctype html><html><head><meta charset=utf-8><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Paper summary on how to generate relevant YouTube recommendations"><base href=https://saikatkumardey.com/><title>Deep Neural Networks for YouTube Recommendations | Saikat Kumar Dey</title><link rel=canonical href=https://saikatkumardey.com/essays/deep-nn-for-youtube-recommendations/><link href=https://saikatkumardey.com/css/style.css type=text/css rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Alegreya&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css?family=Fira+Mono&display=swap" rel=stylesheet><script async defer data-domain=saikatkumardey.com src=https://plausible.io/js/plausible.outbound-links.js></script><meta property="og:title" content="Deep Neural Networks for YouTube Recommendations"><meta property="og:description" content="Paper summary on how to generate relevant YouTube recommendations"><meta property="og:type" content="article"><meta property="og:url" content="https://saikatkumardey.com/essays/deep-nn-for-youtube-recommendations/"><meta property="article:section" content="essays"><meta property="article:published_time" content="2021-08-29T09:59:03+05:30"><meta property="article:modified_time" content="2021-08-29T09:59:03+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deep Neural Networks for YouTube Recommendations"><meta name=twitter:description content="Paper summary on how to generate relevant YouTube recommendations"></head><body><div class=container><header><h1><a href=https://saikatkumardey.com/>Saikat Kumar Dey</a></h1><nav><ul><li><a href=https://saikatkumardey.com/essays/ title>Essays</a></li><li><a href=https://saikatkumardey.com/daily/ title>Daily</a></li></ul></nav></header><div id=content><main class=essay><h1 id=title>Deep Neural Networks for YouTube Recommendations</h1><aside class=meta><div><span class=date>August 29, 2021 â€¢ ~900 words</span><ul class=categories><li><i>in</i></li><li><a href=https://saikatkumardey.com/categories/ml/>ml</a></li></ul></div></aside><div><article id=content><p><img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1629830470549/vXmBYRQ6P.png alt="Screenshot 2021-08-25 at 12.11.06 AM.png"></p><p>YouTube has 100m+ daily active users who consume more than a billion hours&rsquo; worth of content every day. 100s of hours of videos are uploaded every second. At that scale, recommending personalized videos is a colossal task.</p><p>I&rsquo;ve always wondered how YouTube is always able to come up with relevant recommendations that kept me hooked! I found a very interesting paper on <a href=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf>Deep Neural networks for YouTube Recommendations</a>. In this post, I will summarise the key ideas.</p><h2 id=the-problem>The Problem</h2><p>To able to come up with relevant & personalized recommendations for every user is a problem because of:</p><ul><li><strong>scale</strong>: billions of users, billions of videos.</li><li><strong>freshness</strong>: massive volume of videos are uploaded every day. It&rsquo;s an explore-exploit trade-off between popular vs new content.</li><li><strong>noise</strong>: only sparse implicit user feedback is available for modeling.</li></ul><p>In this paper, the authors demonstrate the usage of deep learning techniques for improving recommendations as opposed to matrix-factorization techniques used earlier.</p><h2 id=key-ideas>Key Ideas</h2><ul><li><p>The problem of recommendations at scale is divided into 2 subproblems:</p><ol><li><strong>Candidate Generation</strong> - selects a small subset from the overall corpus which might be relevant to the user.</li><li><strong>Ranking</strong> - ranks the candidate videos based on their relative importance.</li></ol></li><li><p>For Candidate Generation, the objective is to predict the next video watch. User search/watch history, demographics, etc are used by a simple feed-forward network as embeddings which are jointly learned during the training.</p></li><li><p>For Ranking, the objective is to model an expected watch time. A score is assigned based on the expected watch time and videos are sorted accordingly.</p></li><li><p>Similar neural network architecture is used for both procedures.</p></li><li><p>Offline metrics like precision, recall, ranking loss, etc. are used during development. A/B test is used to determine the final effectiveness of the model. We&rsquo;ve already explored the <a href=https://saikatkumardey.com/predictive-model-performance-offline-and-online-evaluations>discrepancies between offline vs online evaluation</a> in a different post.</p></li></ul><h2 id=model-architecture>Model Architecture</h2><h3 id=candidate-generation>Candidate generation</h3><p><img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1630224116536/RukzfmlR1.png alt="Screenshot 2021-08-29 at 1.31.51 PM.png"></p><ul><li><p>** Problem Formulation:** Recommendation is formulated as an <em>extreme multi-class classification</em>
P(w_t = i | U,C) = softmax(v_i . u)
where,
w_t = video v_i watched at time t
U = user
C = context
v_i = dense video embeddings
u = dense user embeddings</p><ul><li>The task of the deep neural network is to learn the embeddings <em>u</em> as a function of user history and context.</li><li>user completing a video watch is a positive example.</li><li><a href=https://www.tensorflow.org/extras/candidate_sampling.pdf>candidate sampling</a> & <a href=https://en.wikipedia.org/wiki/Importance_sampling>importance weighting</a> is used to sample negative examples.</li></ul></li><li><p>Embeddings describing the watch history, user query, demographics, etc are fed to a simple feed-forward neural network having a final softmax layer to learn the class probabilities.</p><ul><li><strong>watch history</strong>: dense vector representation of watched video is learned from a sequence of video-ids (just like word2vec).</li><li><strong>user query</strong>: n-gram representations</li><li><strong>demographics</strong>: geographic region, device, age, etc. are used as numerical/categorical features</li></ul></li><li><p>The embeddings are jointly learned while training the model via gradient descent back-propagation.</p></li><li><p>Age of the video is used to model the time-dependent nature of popular videos. Otherwise, the good old popular videos are going to be selected most of the times, isn&rsquo;t it?</p></li><li><p>What&rsquo;s interesting is that a lot of features were &ldquo;engineered&rdquo; as opposed to the promise of deep learning to reduce it.</p></li><li><p><strong>Training data & label:</strong></p><ul><li>There&rsquo;s an inherent sequence of video consumption. Hence, using random held-out data will be cheating since future information will leak into the training process. The model will overfit! Think about time-series forecasting. A random train-test split won&rsquo;t work since the future data is not available in production during serving time.
<img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1630225522918/zoyaMC-Ae.png alt="Screenshot 2021-08-29 at 1.55.18 PM.png"></li><li>The authors propose a model of predicting <em>user&rsquo;s next watch</em> instead of a randomly held-out watch. This makes sense, as we consume videos in a sequence. For example, if you&rsquo;re watching a series with several episodes, recommending a random episode from that series doesn&rsquo;t make sense.
<img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1630225542842/UAGeaWt8C.png alt="Screenshot 2021-08-29 at 1.55.39 PM.png"></li></ul></li><li><p><strong>Serving:</strong></p><ul><li>To score millions of videos in latency of tens of milliseconds, a nearest neighbor-based search algorithm is used. Exact probability values of softmax() are not required. Hence, a dot product of user and video embeddings could be used to figure out the propensity score of a user <em>u</em> for a particular video <em>v_i</em>. A nearest neighbor search algorithm could be used to figure out the top K candidate videos based on the score.</li></ul></li></ul><h3 id=ranking>Ranking</h3><p><img src=https://cdn.hashnode.com/res/hashnode/image/upload/v1630229225282/zDkuTAg45.png alt="Screenshot 2021-08-29 at 2.56.54 PM.png"></p><ul><li>Candidate generation selects a few hundred out of millions of videos. The ranking procedure could make use of more video features as well as user&rsquo;s interactions with it in order to figure out an order of recommendation.</li><li>The model architecture is similar to the candidate generation procedure. We assign a score to the videos using weighted logistic regression.</li><li>The objective to optimize is a function of expected watch time per impression.</li><li>Why not click-through rate? Well, that would promote clickbait videos instead of quality content. Watch time is a better signal that captures engagement.</li><li><strong>Modeling Expected Watch Time</strong><ul><li><strong>Objective</strong>: Predict expected watch time for a given video.</li><li><strong>Model</strong>: Weighted Logistic Regression, since the class distributions are imbalanced.<ul><li><strong>Positive example</strong>: the video was watched.</li><li><strong>Negative example</strong>: the video was not clicked.</li></ul></li><li>**What are the weights in &ldquo;weighted&rdquo; logistic regression? **<ul><li>Positive examples are weighted by the watch time.</li><li>Negative examples are given a weight of 1.</li></ul></li><li><strong>Loss</strong>: cross-entropy</li></ul></li></ul><h2 id=references>References</h2><ol><li><a href=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf>Deep Neural Networks for YouTube Recommendations</a></li><li><a href=https://en.wikipedia.org/wiki/Softmax_function>Softmax</a></li></ol></article></div></main></div><footer><div><p>&#169; 2022 Saikat Kumar Dey.</p></div></footer></div></body></html>