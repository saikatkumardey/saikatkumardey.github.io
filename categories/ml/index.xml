<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ml on Saikat Kumar Dey</title><link>https://saikatkumardey.com/categories/ml/</link><description>ml</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sat, 25 Jun 2022 13:06:10 +0530</lastBuildDate><atom:link href="https://saikatkumardey.com/categories/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Training Image Classification Models with scikeras</title><link>https://saikatkumardey.com/posts/scikeras/</link><pubDate>Sat, 25 Jun 2022 13:06:10 +0530</pubDate><guid>https://saikatkumardey.com/posts/scikeras/</guid><description>&lt;h2 id="introduction-to-scikeras">Introduction to scikeras&lt;/h2>
&lt;p>Scikeras is a powerful tool that allows users to combine the capabilities of TensorFlow and scikit-learn in order to train image classification models. By using scikeras, users can take advantage of the strengths of both TensorFlow and scikit-learn, and benefit from the ease of use and flexibility of scikit-learn&amp;rsquo;s API.&lt;/p>
&lt;p>In this blog post, we will demonstrate how to use scikeras to train a model using a large dataset of images stored on disk. We will show how to use TensorFlow&amp;rsquo;s &lt;code>ImageDataGenerator()&lt;/code> function to load images in batches and apply real-time augmentation, and how to use scikeras&amp;rsquo;s &lt;code>KerasClassifier()&lt;/code> to create a scikit-learn compatible interface for training the model. We will also demonstrate how to use &lt;code>partial_fit()&lt;/code> to train the model on smaller batches of data and retain the history of model weights and parameters.&lt;/p>
&lt;h2 id="download-dataset">Download dataset&lt;/h2>
&lt;p>Download a sample &lt;a href="https://www.kaggle.com/datasets/muratkokludataset/pistachio-image-dataset">dataset&lt;/a> and store the dataset in &lt;code>data/&lt;/code>. Your directory structure should look like the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>data/Pistachio_Image_Dataset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── Kirmizi_Pistachio/*.jpg
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── Siirt_Pistachio/*.jpg
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="import-necessary-libraries">Import necessary libraries&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a90d91">from&lt;/span> &lt;span style="color:#000">math&lt;/span> &lt;span style="color:#a90d91">import&lt;/span> &lt;span style="color:#000">ceil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a90d91">import&lt;/span> &lt;span style="color:#000">tensorflow&lt;/span> &lt;span style="color:#a90d91">as&lt;/span> &lt;span style="color:#000">tf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a90d91">from&lt;/span> &lt;span style="color:#000">matplotlib&lt;/span> &lt;span style="color:#a90d91">import&lt;/span> &lt;span style="color:#000">pyplot&lt;/span> &lt;span style="color:#a90d91">as&lt;/span> &lt;span style="color:#000">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a90d91">from&lt;/span> &lt;span style="color:#000">scikeras.wrappers&lt;/span> &lt;span style="color:#a90d91">import&lt;/span> &lt;span style="color:#000">KerasClassifier&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a90d91">from&lt;/span> &lt;span style="color:#000">tensorflow.keras.preprocessing.image&lt;/span> &lt;span style="color:#a90d91">import&lt;/span> &lt;span style="color:#000">ImageDataGenerator&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="setup-constants">Setup Constants&lt;/h2>
&lt;p>Next, we need to set some constants that will be used throughout the training process. In this example, we will be using a batch size of 32 and training the model for 10 epochs. You may need to adjust these values depending on your dataset and the performance of your model.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">DATA_DIR&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#c41a16">&amp;#34;data/Pistachio_Image_Dataset&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">BATCH_SIZE&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#1c01ce">32&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">EPOCHS&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#1c01ce">10&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="loader-for-reading-data-in-batches">Loader for reading data in batches&lt;/h2>
&lt;p>One of the key advantages of using scikeras is the ability to train a model using large datasets that do not fit into memory. To do this, we can use TensorFlow&amp;rsquo;s &lt;code>ImageDataGenerator()&lt;/code> function to load images in batches and apply real-time augmentation. This allows us to train the model on smaller chunks of the dataset, without having to load the entire dataset into memory.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">image_generator&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#000">ImageDataGenerator&lt;/span>(&lt;span style="color:#000">rescale&lt;/span>&lt;span style="color:#000">=&lt;/span>&lt;span style="color:#1c01ce">1.0&lt;/span> &lt;span style="color:#000">/&lt;/span> &lt;span style="color:#1c01ce">255&lt;/span>)&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">flow_from_directory&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">DATA_DIR&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">target_size&lt;/span>&lt;span style="color:#000">=&lt;/span>(&lt;span style="color:#1c01ce">32&lt;/span>, &lt;span style="color:#1c01ce">32&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">batch_size&lt;/span>&lt;span style="color:#000">=&lt;/span>&lt;span style="color:#000">BATCH_SIZE&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">class_mode&lt;/span>&lt;span style="color:#000">=&lt;/span>&lt;span style="color:#c41a16">&amp;#34;binary&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">total_images&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#a90d91">len&lt;/span>(&lt;span style="color:#000">image_generator&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">filenames&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">total_batches&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#000">ceil&lt;/span>(&lt;span style="color:#000">total_images&lt;/span> &lt;span style="color:#000">//&lt;/span> &lt;span style="color:#000">BATCH_SIZE&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In this code, we are using ImageDataGenerator() to create a generator that will load the images in DATA_DIR in batches of size BATCH_SIZE, apply a rescaling factor of 1/255, and return the images and labels in a binary format. We then calculate the total number of images in the dataset and the total number of batches. These values will be used later in the training loop.&lt;/p>
&lt;h2 id="define-your-tensorflow-model-architecture">Define your Tensorflow model architecture&lt;/h2>
&lt;p>Next, we need to define the architecture of our TensorFlow model. For the purposes of this example, we will be using a simple shallow-net with a single dense layer. However, you can use any architecture that you prefer, and you can experiment with different architectures to see which one performs best on your dataset.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">model&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#000">tf&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">keras&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">Sequential&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">tf&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">keras&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">layers&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">Input&lt;/span>(&lt;span style="color:#000">shape&lt;/span>&lt;span style="color:#000">=&lt;/span>(&lt;span style="color:#1c01ce">32&lt;/span>, &lt;span style="color:#1c01ce">32&lt;/span>, &lt;span style="color:#1c01ce">3&lt;/span>)),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">tf&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">keras&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">layers&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">Flatten&lt;/span>(),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">tf&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">keras&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">layers&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">Dense&lt;/span>(&lt;span style="color:#1c01ce">1&lt;/span>, &lt;span style="color:#000">activation&lt;/span>&lt;span style="color:#000">=&lt;/span>&lt;span style="color:#c41a16">&amp;#34;sigmoid&amp;#34;&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="define-scikeras-interface">Define scikeras interface&lt;/h2>
&lt;p>Once we have defined our TensorFlow model, we can use scikeras&amp;rsquo;s KerasClassifier() function to create a scikit-learn compatible interface for training the model. This allows us to use the familiar fit() and predict() methods from scikit-learn, while taking advantage of the capabilities of TensorFlow.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">sk_clf&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#000">KerasClassifier&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">model&lt;/span>&lt;span style="color:#000">=&lt;/span>&lt;span style="color:#000">model&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">optimizer&lt;/span>&lt;span style="color:#000">=&lt;/span>&lt;span style="color:#c41a16">&amp;#34;adam&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">loss&lt;/span>&lt;span style="color:#000">=&lt;/span>&lt;span style="color:#c41a16">&amp;#34;binary_crossentropy&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">metrics&lt;/span>&lt;span style="color:#000">=&lt;/span>[&lt;span style="color:#c41a16">&amp;#34;accuracy&amp;#34;&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In this code, we are creating a KerasClassifier object and passing it our TensorFlow model, as well as some other parameters such as the optimizer and loss function to use during training. We are also specifying that we want to track the accuracy metric during training. You can adjust these parameters as needed to suit your specific use.&lt;/p>
&lt;h2 id="training-loop">Training loop&lt;/h2>
&lt;p>Now that we have set up the necessary components for training our model, we can implement the main training loop. This loop will iterate over the batches of images generated by &lt;code>ImageDataGenerator()&lt;/code>, and will use &lt;code>partial_fit()&lt;/code> to train the model on each batch. &lt;code>partial_fit()&lt;/code> has the advantage of allowing us to train the model on smaller batches of data, and it also retains the history of model weights and parameters, whereas &lt;code>fit()&lt;/code> resets this history every time it is called.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">batch&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#1c01ce">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">epoch&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#1c01ce">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">histories&lt;/span> &lt;span style="color:#000">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a90d91">for&lt;/span> &lt;span style="color:#000">X&lt;/span>, &lt;span style="color:#000">y&lt;/span> &lt;span style="color:#000">in&lt;/span> &lt;span style="color:#000">image_generator&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">sk_clf&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">partial_fit&lt;/span>(&lt;span style="color:#000">X&lt;/span>, &lt;span style="color:#000">y&lt;/span>, &lt;span style="color:#000">verbose&lt;/span>&lt;span style="color:#000">=&lt;/span>&lt;span style="color:#a90d91">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">history&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#000">sk_clf&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">model_&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">history&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">history&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">histories&lt;/span>&lt;span style="color:#000">.&lt;/span>&lt;span style="color:#000">append&lt;/span>(&lt;span style="color:#000">history&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">batch&lt;/span> &lt;span style="color:#000">+=&lt;/span> &lt;span style="color:#1c01ce">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a90d91">if&lt;/span> &lt;span style="color:#000">batch&lt;/span> &lt;span style="color:#000">==&lt;/span> &lt;span style="color:#000">total_batches&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">batch&lt;/span> &lt;span style="color:#000">=&lt;/span> &lt;span style="color:#1c01ce">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">epoch&lt;/span> &lt;span style="color:#000">+=&lt;/span> &lt;span style="color:#1c01ce">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a90d91">print&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#c41a16">f&lt;/span>&lt;span style="color:#c41a16">&amp;#34;epoch &lt;/span>&lt;span style="color:#c41a16">{&lt;/span>&lt;span style="color:#000">epoch&lt;/span>&lt;span style="color:#c41a16">}&lt;/span>&lt;span style="color:#c41a16">/&lt;/span>&lt;span style="color:#c41a16">{&lt;/span>&lt;span style="color:#000">EPOCHS&lt;/span>&lt;span style="color:#c41a16">}&lt;/span>&lt;span style="color:#c41a16">, loss &lt;/span>&lt;span style="color:#c41a16">{&lt;/span>&lt;span style="color:#000">history&lt;/span>[&lt;span style="color:#c41a16">&amp;#39;loss&amp;#39;&lt;/span>][&lt;span style="color:#1c01ce">0&lt;/span>]&lt;span style="color:#c41a16">}&lt;/span>&lt;span style="color:#c41a16"> accuracy &lt;/span>&lt;span style="color:#c41a16">{&lt;/span>&lt;span style="color:#000">history&lt;/span>[&lt;span style="color:#c41a16">&amp;#39;accuracy&amp;#39;&lt;/span>][&lt;span style="color:#1c01ce">0&lt;/span>]&lt;span style="color:#c41a16">}&lt;/span>&lt;span style="color:#c41a16">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a90d91">if&lt;/span> &lt;span style="color:#000">epoch&lt;/span> &lt;span style="color:#000">==&lt;/span> &lt;span style="color:#000">EPOCHS&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a90d91">break&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In this code, we are looping over the batches generated by ImageDataGenerator() and calling partial_fit() on each batch. We are also keeping track of the history of model weights and parameters, and we are printing the loss and accuracy for each epoch. Once we have reached the specified number of epochs, the training loop will exit and the model will be trained. At this point, you can use the &lt;code>predict()&lt;/code> method to make predictions on new data, or you can continue training the model using &lt;code>partial_fit()&lt;/code> to improve its performance further.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In this blog post, we have demonstrated how to use scikeras to train a TensorFlow model on a large dataset of images stored on disk. We have shown how to use &lt;code>ImageDataGenerator()&lt;/code> to load images in batches and apply real-time augmentation, how to use &lt;code>KerasClassifier()&lt;/code> to create a scikit-learn compatible interface for training the model, and how to use &lt;code>partial_fit()&lt;/code> to train the model on smaller batches of data and retain the history of model weights and parameters.&lt;/p>
&lt;p>Overall, scikeras is a convenient and effective tool for training image classification models, and it allows users to take advantage of the strengths of both TensorFlow and scikit-learn. There are many potential avenues for further improvement, such as experimenting with different model architectures, fine-tuning the training parameters, and applying more advanced augmentation techniques. We hope that this blog post has provided a useful introduction to scikeras and has given you some ideas for how to use it in your own projects.&lt;/p></description></item><item><title>Accuracy Is a Poor evaluation Metric</title><link>https://saikatkumardey.com/posts/accuracy-is-a-poor-error-metric/</link><pubDate>Tue, 07 Jun 2022 10:25:15 +0530</pubDate><guid>https://saikatkumardey.com/posts/accuracy-is-a-poor-error-metric/</guid><description>&lt;p>For classification problems, something like Cross-entropy is a good metric for minimizing your model&amp;rsquo;s loss.
However, we need something easier to convey to the stakeholders.
Accuracy is a very easy way to show our model performances. The caveat? Accuracy would be misleading for imbalanced classes.&lt;/p>
&lt;p>Here&amp;rsquo;s why.&lt;/p>
&lt;p>Let&amp;rsquo;s say that you&amp;rsquo;ve built a model for detecting cancer.
Here&amp;rsquo;s how the confusion matrix would look like for a dataset on cancer cases.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Predicted Cancer&lt;/th>
&lt;th>Predicted cancer-free&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Actual cancer cases&lt;/td>
&lt;td>10 (TP)&lt;/td>
&lt;td>5 (FN)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Actual cancer-free cases&lt;/td>
&lt;td>150 (FP)&lt;/td>
&lt;td>800 (TN)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The dataset has 950 cancer-free patients and 15 patients with cancer.
The accuracy of the model = fraction of correct predictions = (10 + 800) / (10 + 5 + 150 + 800) = 0.84.&lt;/p>
&lt;p>Well, the cancer cases are quite rare, so it barely has any effect on the accuracy.&lt;/p>
&lt;p>If the model had predicted every case as cancer-free, the confusion matrix would look like the following:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Predicted Cancer&lt;/th>
&lt;th>Predicted cancer-free&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Actual cancer cases&lt;/td>
&lt;td>0 (TP)&lt;/td>
&lt;td>15 (FN)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Actual cancer-free cases&lt;/td>
&lt;td>0 (FP)&lt;/td>
&lt;td>950 (TN)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Accuracy = 950/(950 + 15) = 0.984%.&lt;/p>
&lt;p>That is not what we want to report to our stakeholders.&lt;/p>
&lt;p>What are our options?&lt;/p>
&lt;p>In such cases with class imbalance, we usually look at &lt;em>precision&lt;/em> and &lt;em>recall&lt;/em>.&lt;/p>
&lt;p>&lt;strong>Precision&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>precision = TP/(TP + FP)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Precision is &lt;em>out of all the cancer diagnosis made by the model, how many of them actually have cancer&lt;/em>. In the first case, model predicted 165 cases as having cancer but only 15 of them actually had cancer. So, the precision is 15/165 which is quite low!&lt;/p>
&lt;p>&lt;strong>Recall&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>recall = TP/(TP+FN)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Recall is &lt;em>out of all the actual cancer cases, how many could the model detect?&lt;/em>
In the first case, model detected 15 out of 20 cancer cases. So, the recall is 15/20 = 0.75. That&amp;rsquo;s not bad!&lt;/p>
&lt;p>Our objective is to detect as many cancer cases as we can (TP) while minimizing the false diagnosis of cancer (FP). In other words, we want to have high recall with high precision.&lt;/p>
&lt;p>All of these metrics rely on the probability threshold chosen to classify each sample as having cancer or not. If prediction probability &amp;gt; threshold, predict cancer.&lt;/p>
&lt;p>For example, if you choose 0.9 as the threshold, you might only classify cases where the model is highly confident ie, high precision. However, you might miss out on many cases at that threshold ie, low recall. Instead, if you keep 0.1 as the threshold, you might capture all the cancer cases but you&amp;rsquo;d be scaring a lot of other cancer-free patients with misdiagnosis as well.&lt;/p>
&lt;p>By varying the probability threshold, you could check the trade-off between precision &amp;amp; recall, also called the precision-recall curve.&lt;/p>
&lt;p>Sometimes, precision &amp;amp; recall are combined together into one metric, called F1 score.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>F1 = 2 / (1/precision + 1/recall)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There&amp;rsquo;s another variant of the precision-recall curve which is agnostic to the probability threshold selected. It&amp;rsquo;s called the ROC curve (Receiver Operating Characteristics). The area under the curve (AUC) is demonstrative of the overall model performance. AUC is useful to know how well the model is in classifying both the classes.&lt;/p>
&lt;p>&lt;strong>Conclusion&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Do not use accuracy metric when there&amp;rsquo;s a class imbalance.&lt;/li>
&lt;li>Use F1 or ROC-AUC instead.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>&lt;strong>Reference&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://www.oreilly.com/library/view/practical-machine-learning/9781098102357/">Practical Machine Learning for Computer Vision&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision Recall&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC Curve&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Deep Neural Networks for YouTube Recommendations</title><link>https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/</link><pubDate>Sun, 29 Aug 2021 09:59:03 +0530</pubDate><guid>https://saikatkumardey.com/posts/deep-nn-for-youtube-recommendations/</guid><description>&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1629830470549/vXmBYRQ6P.png" alt="Screenshot 2021-08-25 at 12.11.06 AM.png">&lt;/p>
&lt;p>YouTube has 100m+ daily active users who consume more than a billion hours&amp;rsquo; worth of content every day. 100s of hours of videos are uploaded every second. At that scale, recommending personalized videos is a colossal task.&lt;/p>
&lt;p>I&amp;rsquo;ve always wondered how YouTube is always able to come up with relevant recommendations that kept me hooked! I found a very interesting paper on &lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf">Deep Neural networks for YouTube Recommendations&lt;/a>. In this post, I will summarise the key ideas.&lt;/p>
&lt;h2 id="the-problem">The Problem&lt;/h2>
&lt;p>To able to come up with relevant &amp;amp; personalized recommendations for every user is a problem because of:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>scale&lt;/strong>: billions of users, billions of videos.&lt;/li>
&lt;li>&lt;strong>freshness&lt;/strong>: massive volume of videos are uploaded every day. It&amp;rsquo;s an explore-exploit trade-off between popular vs new content.&lt;/li>
&lt;li>&lt;strong>noise&lt;/strong>: only sparse implicit user feedback is available for modeling.&lt;/li>
&lt;/ul>
&lt;p>In this paper, the authors demonstrate the usage of deep learning techniques for improving recommendations as opposed to matrix-factorization techniques used earlier.&lt;/p>
&lt;h2 id="key-ideas">Key Ideas&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>The problem of recommendations at scale is divided into 2 subproblems:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Candidate Generation&lt;/strong> - selects a small subset from the overall corpus which might be relevant to the user.&lt;/li>
&lt;li>&lt;strong>Ranking&lt;/strong> - ranks the candidate videos based on their relative importance.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>For Candidate Generation, the objective is to predict the next video watch. User search/watch history, demographics, etc are used by a simple feed-forward network as embeddings which are jointly learned during the training.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For Ranking, the objective is to model an expected watch time. A score is assigned based on the expected watch time and videos are sorted accordingly.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Similar neural network architecture is used for both procedures.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Offline metrics like precision, recall, ranking loss, etc. are used during development. A/B test is used to determine the final effectiveness of the model. We&amp;rsquo;ve already explored the &lt;a href="https://saikatkumardey.com/predictive-model-performance-offline-and-online-evaluations">discrepancies between offline vs online evaluation&lt;/a> in a different post.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;h3 id="candidate-generation">Candidate generation&lt;/h3>
&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1630224116536/RukzfmlR1.png" alt="Screenshot 2021-08-29 at 1.31.51 PM.png">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>** Problem Formulation:** Recommendation is formulated as an &lt;em>extreme multi-class classification&lt;/em>
P(w_t = i | U,C) = softmax(v_i . u)
where,
w_t = video v_i watched at time t
U = user
C = context
v_i = dense video embeddings
u = dense user embeddings&lt;/p>
&lt;ul>
&lt;li>The task of the deep neural network is to learn the embeddings &lt;em>u&lt;/em> as a function of user history and context.&lt;/li>
&lt;li>user completing a video watch is a positive example.&lt;/li>
&lt;li>&lt;a href="https://www.tensorflow.org/extras/candidate_sampling.pdf">candidate sampling&lt;/a> &amp;amp; &lt;a href="https://en.wikipedia.org/wiki/Importance_sampling">importance weighting&lt;/a> is used to sample negative examples.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Embeddings describing the watch history, user query, demographics, etc are fed to a simple feed-forward neural network having a final softmax layer to learn the class probabilities.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>watch history&lt;/strong>: dense vector representation of watched video is learned from a sequence of video-ids (just like word2vec).&lt;/li>
&lt;li>&lt;strong>user query&lt;/strong>: n-gram representations&lt;/li>
&lt;li>&lt;strong>demographics&lt;/strong>: geographic region, device, age, etc. are used as numerical/categorical features&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>The embeddings are jointly learned while training the model via gradient descent back-propagation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Age of the video is used to model the time-dependent nature of popular videos. Otherwise, the good old popular videos are going to be selected most of the times, isn&amp;rsquo;t it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>What&amp;rsquo;s interesting is that a lot of features were &amp;ldquo;engineered&amp;rdquo; as opposed to the promise of deep learning to reduce it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Training data &amp;amp; label:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>There&amp;rsquo;s an inherent sequence of video consumption. Hence, using random held-out data will be cheating since future information will leak into the training process. The model will overfit! Think about time-series forecasting. A random train-test split won&amp;rsquo;t work since the future data is not available in production during serving time.
&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1630225522918/zoyaMC-Ae.png" alt="Screenshot 2021-08-29 at 1.55.18 PM.png">&lt;/li>
&lt;li>The authors propose a model of predicting &lt;em>user&amp;rsquo;s next watch&lt;/em> instead of a randomly held-out watch. This makes sense, as we consume videos in a sequence. For example, if you&amp;rsquo;re watching a series with several episodes, recommending a random episode from that series doesn&amp;rsquo;t make sense.
&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1630225542842/UAGeaWt8C.png" alt="Screenshot 2021-08-29 at 1.55.39 PM.png">&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Serving:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>To score millions of videos in latency of tens of milliseconds, a nearest neighbor-based search algorithm is used. Exact probability values of softmax() are not required. Hence, a dot product of user and video embeddings could be used to figure out the propensity score of a user &lt;em>u&lt;/em> for a particular video &lt;em>v_i&lt;/em>. A nearest neighbor search algorithm could be used to figure out the top K candidate videos based on the score.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="ranking">Ranking&lt;/h3>
&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1630229225282/zDkuTAg45.png" alt="Screenshot 2021-08-29 at 2.56.54 PM.png">&lt;/p>
&lt;ul>
&lt;li>Candidate generation selects a few hundred out of millions of videos. The ranking procedure could make use of more video features as well as user&amp;rsquo;s interactions with it in order to figure out an order of recommendation.&lt;/li>
&lt;li>The model architecture is similar to the candidate generation procedure. We assign a score to the videos using weighted logistic regression.&lt;/li>
&lt;li>The objective to optimize is a function of expected watch time per impression.&lt;/li>
&lt;li>Why not click-through rate? Well, that would promote clickbait videos instead of quality content. Watch time is a better signal that captures engagement.&lt;/li>
&lt;li>&lt;strong>Modeling Expected Watch Time&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Objective&lt;/strong>: Predict expected watch time for a given video.&lt;/li>
&lt;li>&lt;strong>Model&lt;/strong>: Weighted Logistic Regression, since the class distributions are imbalanced.
&lt;ul>
&lt;li>&lt;strong>Positive example&lt;/strong>: the video was watched.&lt;/li>
&lt;li>&lt;strong>Negative example&lt;/strong>: the video was not clicked.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>**What are the weights in &amp;ldquo;weighted&amp;rdquo; logistic regression? **
&lt;ul>
&lt;li>Positive examples are weighted by the watch time.&lt;/li>
&lt;li>Negative examples are given a weight of 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Loss&lt;/strong>: cross-entropy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf">Deep Neural Networks for YouTube Recommendations&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Predictive Model Performance Online and Offline Eval</title><link>https://saikatkumardey.com/posts/predictive-model-performance-online-and-offline-eval/</link><pubDate>Wed, 25 Aug 2021 09:57:00 +0530</pubDate><guid>https://saikatkumardey.com/posts/predictive-model-performance-online-and-offline-eval/</guid><description>&lt;p>Evaluation is an important topic in every machine learning project. There are offline metrics that we compute on the historical data. It&amp;rsquo;s supposed to provide us an indication of our model performance on real data. However, we often see a discrepancy in offline vs online performance.&lt;/p>
&lt;p>In &lt;a href="https://dl.acm.org/doi/abs/10.1145/2487575.2488215">Predictive model performance: Offline and online evaluations&lt;/a>, the authors investigate the offline/online model performance on advertisement data from Bing Search Engine.&lt;/p>
&lt;h2 id="key-takeaways">Key takeaways&lt;/h2>
&lt;ul>
&lt;li>Evaluation metrics are important since it guides what the model optimizes on. Tuning on incorrect metrics might provide misleading results in offline settings that might surprise us in production.&lt;/li>
&lt;li>AUC is a really good metric to determine model classification efficiency.&lt;/li>
&lt;li>Offline evaluation using AUC doesn&amp;rsquo;t correlate to the online evaluation via A/B tests.&lt;/li>
&lt;li>The authors propose the usage of a simulation metric that simulates user behavior based on historical logs, which works better in the online evaluation.&lt;/li>
&lt;/ul>
&lt;h2 id="details">Details&lt;/h2>
&lt;p>&lt;strong>What is an offline evaluation?&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>In typical ML projects, we split our dataset into train/test sets.&lt;/li>
&lt;li>Models are trained on the train set.&lt;/li>
&lt;li>Evaluation is done on the test set.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What is an online evaluation?&lt;/strong>&lt;/p>
&lt;p>When our model is in production, we perform an A/B test.
Typically, it has 2 variants.&lt;/p>
&lt;ul>
&lt;li>&lt;em>control&lt;/em> group with our existing model&lt;/li>
&lt;li>&lt;em>test&lt;/em> group with the new model.
Live traffic is split into the two groups &amp;amp; metrics like conversion rate, revenue per visitor, etc are measured. If the new model&amp;rsquo;s performance is statistically significant, it&amp;rsquo;s selected for launch.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>The issue?&lt;/strong>&lt;/p>
&lt;p>Offline performance doesn&amp;rsquo;t always correlate to online performance due to the dynamic nature of the latter.&lt;/p>
&lt;h2 id="evaluation-metrics">Evaluation Metrics&lt;/h2>
&lt;ul>
&lt;li>The paper focuses on metrics used for click prediction problems that most search engines like Google, Bing, etc. face.&lt;/li>
&lt;li>Click prediction problems estimates the CTRs of ads given a query.&lt;/li>
&lt;li>This is treated as a binary classification problem.&lt;/li>
&lt;/ul>
&lt;p>We&amp;rsquo;ll review some important evaluation metrics for the use case.&lt;/p>
&lt;h3 id="auc-area-under-curve">AUC (Area under curve)&lt;/h3>
&lt;ul>
&lt;li>Let&amp;rsquo;s say that we have a binary classifier that predicts a probability &lt;em>p&lt;/em> for an event to occur. Then, &lt;em>1-p&lt;/em> is the probability that the event doesn&amp;rsquo;t occur. We need a threshold to determine the class membership. AUC provides a single score that tells us how good a model is across all possible ranges of thresholds.&lt;/li>
&lt;li>AUC is computed from a ROC (Receiver Operating Characteristics) curve.&lt;/li>
&lt;li>ROC curve = a graphical representation of TPR (true positive rate) as a function of FPR (false positive rate) of a binary classifier across different thresholds.&lt;/li>
&lt;/ul>
&lt;h3 id="rig-relative-information-gain">RIG (Relative Information Gain)&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>RIG = 1 - log_loss/entropy(y)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>where,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>log_loss = - [ c*log(p) + (1-c)*log(1-p) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>entropy(y) = - [ y*logy + (1-y)*log(1-y) ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>c = observed click
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>p = probability of a click
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y = CTR
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Higher is better.&lt;/p>
&lt;h3 id="prediction-error-pe">Prediction Error (PE)&lt;/h3>
&lt;p>&lt;code>PE = avg(p)/y - 1&lt;/code>&lt;/p>
&lt;ul>
&lt;li>PE = 0 when average(p) exactly matches the click-through rate.&lt;/li>
&lt;li>It could also be 0 if there&amp;rsquo;s a mix of over-estimation/under-estimation of the CTR as long the average is closer to CTR.&lt;/li>
&lt;li>It&amp;rsquo;s not a reliable metric.&lt;/li>
&lt;/ul>
&lt;h3 id="simulation-metric">Simulation Metric&lt;/h3>
&lt;p>This part is really important. It teaches us a way to simulate different model performances offline without having to run expensive A/B tests.&lt;/p>
&lt;ul>
&lt;li>A/B tests run with a fixed set of model parameters.
&lt;ul>
&lt;li>It could be expensive to run multiple experiments with different model parameters.&lt;/li>
&lt;li>It could also ruin the user experience, make losses if the new model underperforms&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The paper proposes a simulation of the user behavior offline aka auction simulation.&lt;/li>
&lt;li>Auction simulation reruns ad auctions offline for a given query and selects a set of ads based on the new model prediction scores.&lt;/li>
&lt;li>user clicks are estimated in the following way:
&lt;ul>
&lt;li>if (user, ad) pair is found in the logs
&lt;ul>
&lt;li>if it&amp;rsquo;s in the same position in history as in the simulation, use the historic CTR directly as the expected CTR&lt;/li>
&lt;li>if it&amp;rsquo;s not in the same position, the expected CTR is calibrated based on the position.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>if (user, ad) pair is not found, average CTR is used as the expected CTR.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="issues">Issues&lt;/h2>
&lt;h3 id="auc">AUC&lt;/h3>
&lt;ul>
&lt;li>ignores predicted probability values. It&amp;rsquo;s insensitive to the ranking based on the probability score. It&amp;rsquo;s possible to have different rankings with similar AUC scores.&lt;/li>
&lt;li>summarizes the test performance over the entire range of the ROC space, even where one would rarely operate on. Higher ROC doesn&amp;rsquo;t mean a better ranking.&lt;/li>
&lt;li>It weights false-positive and false negatives equally. In real life, the cost of not showing a relevant ad (false negatives) is way more than showing a sub-optimal ad (false positive).&lt;/li>
&lt;li>highly dependent on the underlying data distribution.&lt;/li>
&lt;/ul>
&lt;h3 id="rig">RIG&lt;/h3>
&lt;ul>
&lt;li>Highly sensitive to underlying data distribution.&lt;/li>
&lt;li>We can&amp;rsquo;t judge a model by just using RIG alone.&lt;/li>
&lt;li>We could compare the relative performance of different models trained/tested on the same data.&lt;/li>
&lt;/ul>
&lt;h2 id="offline-vs-online-discrepancy">Offline vs Online discrepancy&lt;/h2>
&lt;p>The authors compare 2 models&lt;/p>
&lt;ul>
&lt;li>model 1 (baseline): tuned on offline metrics like AUC &amp;amp; RIG&lt;/li>
&lt;li>model 2 (test): tuned on the simulation metric&lt;/li>
&lt;/ul>
&lt;p>The finding: model performs well on offline metrics but has a significant dip on online metrics.&lt;/p>
&lt;p>&lt;strong>Why do we see this?&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1629743092351/g_2-BgJ8G.png" alt="Screenshot 2021-08-23 at 11.54.47 PM.png">&lt;/p>
&lt;ul>
&lt;li>Tuning a model on offline metrics like AUC/RIG over-estimates the probability scores at the lower end of the score range.&lt;/li>
&lt;li>Over-estimation of the probability score at the higher end of the score range doesn&amp;rsquo;t matter much since they&amp;rsquo;ll be selected by either model.&lt;/li>
&lt;li>Over-estimation at the lower end of the score range is bad since irrelevant ads are more likely to be shown in that case.&lt;/li>
&lt;li>Offline metrics like AUC/RIG provide an overall score based on the entire range of probability scores - they&amp;rsquo;re not able to capture the intended effect.&lt;/li>
&lt;li>Tuning a model based on the simulation metric correlates better with online performance tests via A/B tests.&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2>
&lt;p>&lt;a href="https://dl.acm.org/doi/abs/10.1145/2487575.2488215">Predictive Model Performance: Offline and Online Evaluations&lt;/a>&lt;/p></description></item></channel></rss>